{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5d9bf4-b3d0-4f92-aba7-1839478c0b79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T11:43:20.853533Z",
     "start_time": "2024-07-09T11:43:19.427691Z"
    }
   },
   "outputs": [],
   "source": [
    "### import all modules\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "import copy\n",
    "from openbabel import openbabel\n",
    "from openbabel import pybel\n",
    "import openbabel as ob\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import Draw\n",
    "#from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit.Chem import rdDistGeom\n",
    "import re\n",
    "import multiprocessing\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32f9b21-41f9-4a91-b164-7cc2d1fb01d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T11:43:20.903442Z",
     "start_time": "2024-07-09T11:43:20.868784Z"
    }
   },
   "outputs": [],
   "source": [
    "### user specified inputs & check if the csv is read in correctly\n",
    "### all the .ct files of the fragemnts\n",
    "### the .csv file with all the properties for each of the node\n",
    "\n",
    "charge_specificed = 0                     ### total charge of the species generated\n",
    "monomers = 2                              ### max number of repeated nodes from the .csv file to include in the species\n",
    "donor_M_connection = 6                    ### additional setting for the max number of connections with M and donor\n",
    "donor_as_bridgeing = False                ### True if the donor ligand used to describe charged multidentate ligand.\n",
    "center = ['Mg', 'Na']                     ### metal centers to filter out 3D structures with atoms too close to the metal atoms. \n",
    "df = pd.read_csv('components-data.csv')   ### name of the fragment property .csv file\n",
    "user_specified_atomcount_controll = []    ### [('componenet1', 2), ('componenet2',1)] to controll the max number of a fragment's presence\n",
    "bonds_not_constructed = [('Na', 'R')]     ### [('componenet1', 'componenet2'), (.. , ..)]\n",
    "num_cores = 7                             ### number of cores to be used, 7 for an 8 core computer etc.\n",
    "dummy_atom_needed = ['Mg', 'Li']          ### ['real atom', 'dummy atom'] \n",
    "\n",
    "\n",
    "### print out the read .csv to confirm the inputs\n",
    "#for index, col in df.iterrows():\n",
    "    #print(col['components'])\n",
    "from ast import literal_eval\n",
    "#df['connectivity allowed2'] = df['connectivity allowed'].apply(lambda cell: list(literal_eval(cell)))\n",
    "df['connectivity allowed'] = df['connectivity allowed'].apply(lambda cell: [int(el) for el in cell.split(', ')])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fb9c5b-5b98-4f9f-ba8d-d210a5e4e29d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T11:43:20.869305Z",
     "start_time": "2024-07-09T11:43:20.859584Z"
    }
   },
   "outputs": [],
   "source": [
    "### a function to plot graphs, called later on\n",
    "def plot_graphs(graphs):\n",
    "    \"\"\"Utility to plot a lot of graphs from an array of graphs. \n",
    "    Each graphs is a list of edges; each edge is a tuple.\"\"\"\n",
    "    figsize=20\n",
    "    dotsize=20\n",
    "    n = len(graphs)\n",
    "    GRAPHS = []\n",
    "    fig = plt.figure(figsize=(figsize,figsize))\n",
    "    fig.patch.set_facecolor('white') # To make copying possible (no transparent background)\n",
    "    k = int(np.sqrt(n))\n",
    "\n",
    "\n",
    "    G = nx.Graph(label=\"x\")\n",
    "    for i in range(0, len(nodes)):\n",
    "        #print(i)\n",
    "        G.add_nodes_from([\n",
    "            (i, {\"label\": nodes[i]}),\n",
    "            ])\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(n):\n",
    "        plt.subplot(k+1,k+1,i+1)\n",
    "        Edges = list(G.edges)\n",
    "        G.remove_edges_from(Edges)\n",
    "        Nodes = list(G.nodes)\n",
    "        G.remove_nodes_from(Nodes)\n",
    "    \n",
    "        for edge in graphs[i]:\n",
    "            G.add_edge(edge[0],edge[1])\n",
    "        \n",
    "        for node in list(G.nodes):\n",
    "            G.add_nodes_from([\n",
    "                (node, {'label': nodes[node]}),\n",
    "                ])\n",
    "    \n",
    "        Gtemp = G.copy()\n",
    "        GRAPHS.append(Gtemp)\n",
    "    \n",
    "        labels = nx.get_node_attributes(G,'label')\n",
    "        pos = nx.spring_layout(G, scale=5, k=5/np.sqrt(G.order()))\n",
    "        nx.draw_networkx(G, pos, node_color='lightblue', node_size=500, labels = labels, width = 2, font_size=13, alpha=1, edge_color=\"black\")\n",
    "        #nx.draw_kamada_kawai(G, node_size=dotsize)\n",
    "        #print('.', i, end='')\n",
    "        \n",
    "        print('.', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cd2700-758f-4e36-b6e6-dc8ea0981392",
   "metadata": {},
   "outputs": [],
   "source": [
    "### get the total charge of the donor ligand\n",
    "\n",
    "nodes = []\n",
    "charges = []\n",
    "donor_atoms = []\n",
    "for index, col in df.iterrows():\n",
    "    nodes.append(col['components'])\n",
    "    charges.append(col['charge'])\n",
    "    donor_atoms.append(col['donor atom'])\n",
    "\n",
    "donor_nodx_atoms = []\n",
    "donor_total_charge = 0\n",
    "for node in nodes:\n",
    "    if 'Donor' in node:\n",
    "        donor_total_charge = donor_total_charge + charges[nodes.index(node)]\n",
    "        donor_nodx_atoms.append(donor_atoms[nodes.index(node)])\n",
    "print(donor_total_charge)\n",
    "#print(donor_nodx_atoms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1796acfa-e9f8-40ca-a445-be187dc15bd4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T11:43:20.904269Z",
     "start_time": "2024-07-09T11:43:20.896580Z"
    }
   },
   "outputs": [],
   "source": [
    "### generating all the nodes\n",
    "\n",
    "nodes = []\n",
    "connectivity_allowed = []\n",
    "charges = []\n",
    "donor_atoms = []\n",
    "type =[]\n",
    "for i in range (0, monomers):\n",
    "    for index, col in df.iterrows():\n",
    "        nodes.append(col['components'])\n",
    "        charges.append(col['charge'])\n",
    "        connectivity_allowed.append(col['connectivity allowed'])\n",
    "        donor_atoms.append(col['donor atom'])\n",
    "        type.append(col['type'])\n",
    "print(nodes)\n",
    "print(charges)\n",
    "print(connectivity_allowed)\n",
    "print(type)\n",
    "\n",
    "if dummy_atom_needed != []:\n",
    "    donor_atoms = [dummy_atom_needed[1] if t == dummy_atom_needed[0] else t for t in donor_atoms]\n",
    "print(donor_atoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb39d8e-a375-4204-ac24-0dc7ed5468fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### check if the donor ligand (otherwords multidentate liand is charged or not)\n",
    "\n",
    "Donorligand_charged = False\n",
    "for node in nodes:\n",
    "    if 'Donor' in node:\n",
    "        Donorligand_charge = charges[nodes.index(node)]\n",
    "        if Donorligand_charge != 0:\n",
    "            Donorligand_charged = True\n",
    "            break\n",
    "print (Donorligand_charged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380b7f15-652b-417e-8ffe-f652650a754f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T11:43:20.924193Z",
     "start_time": "2024-07-09T11:43:20.904124Z"
    }
   },
   "outputs": [],
   "source": [
    "### here is building the edges\n",
    "\n",
    "import itertools\n",
    "n = len(nodes)\n",
    "out = []\n",
    "all_possible_edges = []\n",
    "for i, j in itertools.combinations(list(range(n)), 2):\n",
    "    all_possible_edges.append((i,j))\n",
    "print(all_possible_edges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a874a792-abd3-49ac-b7ee-ab0a2a856b0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T11:43:20.924871Z",
     "start_time": "2024-07-09T11:43:20.910987Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### filter out unwanted edges between 2 atoms\n",
    "\n",
    "import copy\n",
    "all_possible_edges_filtered = copy.copy(all_possible_edges)\n",
    "\n",
    "\n",
    "for edge in all_possible_edges:\n",
    "    if type[edge[0]] == type[edge[1]]:\n",
    "        i=all_possible_edges.index(edge)\n",
    "        all_possible_edges_filtered[i] = 'tbr'\n",
    "    for n in bonds_not_constructed:\n",
    "        if nodes[edge[0]] in n and nodes[edge[1]] in n:\n",
    "            i=all_possible_edges.index(edge) \n",
    "            all_possible_edges_filtered[i] = 'tbr'\n",
    "\n",
    "#print(all_possible_edges_filtered)\n",
    "\n",
    "all_possible_edges_filtered2 = []\n",
    "for edges in all_possible_edges_filtered:\n",
    "    if edges != 'tbr':\n",
    "        all_possible_edges_filtered2.append(edges)\n",
    "\n",
    "print(all_possible_edges_filtered2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d196a1-d69b-4afa-8101-0741309f22f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T11:43:20.925112Z",
     "start_time": "2024-07-09T11:43:20.919559Z"
    }
   },
   "outputs": [],
   "source": [
    "### here we would like to group the donor ligand nodes and the non-donor ligand nodes to reduce the combinatorial explosion\n",
    "\n",
    "edges_nodonor = []\n",
    "edges_donor = []\n",
    "for edges in all_possible_edges_filtered2:\n",
    "    \n",
    "    if 'Donor' not in nodes[edges[0]] and 'Donor' not in nodes[edges[1]]:\n",
    "        edges_nodonor.append(edges)\n",
    "    else:\n",
    "        edges_donor.append(edges)\n",
    "print(edges_nodonor)\n",
    "print(edges_donor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a809f3-2845-486b-82d7-1bb84e3e0b6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T11:43:21.029710Z",
     "start_time": "2024-07-09T11:43:20.926676Z"
    }
   },
   "outputs": [],
   "source": [
    "#************************************************* non donor graphs ****************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d55697-d0ef-406d-ad83-c4f61032371c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### generating combinations of non donor edges\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import itertools\n",
    "import multiprocessing\n",
    "\n",
    "def generate_combinations(length, max_index):\n",
    "    \"\"\"\n",
    "    Generate all combinations of a given length.\n",
    "    \"\"\"\n",
    "    result = list(itertools.combinations(range(max_index), length))\n",
    "    print(f\"[PID {multiprocessing.current_process().pid}] Finished length {length} with {len(result)} combinations\")\n",
    "    return result\n",
    "\n",
    "\n",
    "max_index = len(edges_nodonor)\n",
    "\n",
    "# Parallelize with basic print-based progress tracking\n",
    "results = Parallel(n_jobs=num_cores)(\n",
    "    delayed(generate_combinations)(length, max_index) for length in range(1, max_index + 1)\n",
    ")\n",
    "\n",
    "# Flatten\n",
    "all_possible_combinations = [item for sublist in results for item in sublist]\n",
    "\n",
    "print(\"Total combinations:\", len(all_possible_combinations))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2772d2e7-08de-4f84-b26c-bac07e9dd896",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T11:43:21.036864Z",
     "start_time": "2024-07-09T11:43:20.963117Z"
    }
   },
   "outputs": [],
   "source": [
    "### now we make all combinations of no donor containing components into graphs\n",
    "\n",
    "tempall_possible_nodonor_graphs = []\n",
    "for combinations in all_possible_combinations:\n",
    "    edges = []\n",
    "    #print (combinations)\n",
    "    for edge in combinations:\n",
    "        #print (edge)\n",
    "        edges.append(edges_nodonor[int(edge)])\n",
    "    tempall_possible_nodonor_graphs.append(edges)\n",
    "print(len(tempall_possible_nodonor_graphs))\n",
    "\n",
    "\n",
    "G = nx.Graph(label=\"x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e67ad6d-d8d5-4d06-a562-30c29c4d59af",
   "metadata": {},
   "outputs": [],
   "source": [
    "### and filter them by connection\n",
    "\n",
    "import copy\n",
    "filtered2_nodonor_combinations = copy.copy(tempall_possible_nodonor_graphs)\n",
    "\n",
    "for i in range(0, len(tempall_possible_nodonor_graphs)):\n",
    "    G.clear()\n",
    "    #print(i)\n",
    "    \n",
    "    for edge in tempall_possible_nodonor_graphs[i]:            \n",
    "        G.add_edge(edge[0],edge[1])\n",
    "        \n",
    "    for node in list(G.nodes):\n",
    "        degrees = nx.degree(G, nbunch=node)\n",
    "        #print(node)    \n",
    "        if degrees not in connectivity_allowed[node]:\n",
    "            #print(degrees)\n",
    "            filtered2_nodonor_combinations[i] = 'tbr'\n",
    "            break\n",
    "            \n",
    "filtered3_nodonor_combinations = []\n",
    "for edge in filtered2_nodonor_combinations:\n",
    "    if edge != 'tbr':\n",
    "        filtered3_nodonor_combinations.append(edge)\n",
    "\n",
    "print (len(filtered3_nodonor_combinations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbf926a-e5e4-45d1-9d45-30ec6ccf5190",
   "metadata": {},
   "outputs": [],
   "source": [
    "### continue filter by charge, if donor ligand is not charged\n",
    "\n",
    "filtered4_nodonor_combinations = []\n",
    "filteredcomplete_nodonor_combinations = []\n",
    "\n",
    "if Donorligand_charged == True:\n",
    "    filtered4_nodonor_combinations = filtered3_nodonor_combinations  # No need to filter further\n",
    "    print('no charge filter')\n",
    "else:\n",
    "    print('yes charge filter')\n",
    "    for i in range(0, len(filtered3_nodonor_combinations)):\n",
    "        Edges = list(G.edges)  \n",
    "        G.clear()\n",
    "        for edge in filtered3_nodonor_combinations[i]:            \n",
    "            G.add_edge(edge[0], edge[1])\n",
    "        charge = 0 \n",
    "\n",
    "        for node in list(G.nodes):\n",
    "            charge = charge + charges[node] \n",
    "        # Now filter based on specified charge\n",
    "        if charge == charge_specificed:\n",
    "            filtered4_nodonor_combinations.append(filtered3_nodonor_combinations[i])\n",
    "\n",
    "print(len(filtered4_nodonor_combinations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6f9afb-7a43-45f8-ac23-2eb05184d7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### filter out the isomorph graphs \n",
    "\n",
    "graphs = []\n",
    "filtered5_nodonor_combinations = []\n",
    "filtered5_nodonor_graphs = []\n",
    "\n",
    "G = nx.Graph(label=\"x\")\n",
    "for i in range(0, len(nodes)):\n",
    "    #print(i)\n",
    "    G.add_nodes_from([\n",
    "        (i, {\"label\": nodes[i]}),\n",
    "        ])\n",
    "\n",
    "for i in range(0, len(filtered4_nodonor_combinations)):\n",
    "    Edges = list(G.edges)\n",
    "    G.remove_edges_from(Edges)\n",
    "    \n",
    "    for edge in filtered4_nodonor_combinations[i]:            \n",
    "        G.add_edge(edge[0],edge[1])\n",
    "    \n",
    "    graphs.append(nx.weisfeiler_lehman_graph_hash(G, node_attr='label'))\n",
    "\n",
    "\n",
    "for i in range(0, len(graphs)):\n",
    "    if graphs[i] not in filtered5_nodonor_graphs:\n",
    "        filtered5_nodonor_combinations.append(filtered4_nodonor_combinations[i])\n",
    "        filtered5_nodonor_graphs.append(graphs[i])\n",
    "\n",
    "\n",
    "filteredcomplete_nodonor_combinations = filtered5_nodonor_combinations\n",
    "\n",
    "print (len(filteredcomplete_nodonor_combinations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236d4cd9-8743-4210-b768-a46224ad90c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### user specificed controll of atom number filter if specified above:\n",
    "\n",
    "filtered6_nodonor_combinations = []\n",
    "filtered_edges = copy.copy(filtered5_nodonor_combinations)\n",
    "\n",
    "if user_specified_atomcount_controll:\n",
    "    print ('now filter according to user defined atom_num')\n",
    "    for i in range(0, len(filtered5_nodonor_combinations)):\n",
    "        Edges = list(G.edges)\n",
    "        G.clear()\n",
    "        G.remove_edges_from(Edges)\n",
    "        for edge in filtered5_nodonor_combinations[i]:            \n",
    "            G.add_edge(edge[0],edge[1])\n",
    "        charge = 0\n",
    "    \n",
    "        #print(list(G.nodes))\n",
    "\n",
    "        for atom, num in user_specified_atomcount_controll:\n",
    "            count = 0\n",
    "            for node in list(G.nodes):\n",
    "                if nodes[node] == atom:\n",
    "                    count+=1\n",
    "            if count != num:\n",
    "                filtered_edges[i] = 'tbr'\n",
    "                break\n",
    "\n",
    "    for edges in filtered_edges:\n",
    "        #print (edges)\n",
    "        if edges != 'tbr':\n",
    "            filtered6_nodonor_combinations.append(edges)\n",
    "            #print (filtered6_nodonor_combinations)\n",
    "        \n",
    "    filteredcomplete_nodonor_combinations = copy.copy(filtered6_nodonor_combinations)\n",
    "print (len(filteredcomplete_nodonor_combinations))\n",
    "#print (filteredcomplete_nodonor_combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe38d106-a30d-4ebd-a55c-263df5320a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "### if the donor set as charged multidentate ligands then no structure should be presented here to satisfy charge\n",
    "\n",
    "if donor_as_bridgeing:\n",
    "    filteredcomplete_nodonor_combinations.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71f3ed8-225a-4e3a-8c8f-2fe144487b27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T11:43:23.226806Z",
     "start_time": "2024-07-09T11:43:20.989909Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### plot and check a subset of the graphs\n",
    "\n",
    "plot_graphs(filteredcomplete_nodonor_combinations[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d313480-1edd-4a97-92cc-ef8178b8452c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T11:43:23.236109Z",
     "start_time": "2024-07-09T11:43:23.227470Z"
    }
   },
   "outputs": [],
   "source": [
    "#************************************************* donor graphs ****************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fea5d99-5ca7-44a6-accc-edd2683b1392",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### check if there maybe a combinatorial explosion\n",
    "\n",
    "combinatorial_explosion = False\n",
    "\n",
    "temp_all_possible_combinations = []\n",
    "for node in nodes:\n",
    "    if 'Donor' in node:\n",
    "        if type[nodes.index(node)] == 'metal': \n",
    "            combinatorial_explosion = True\n",
    "\n",
    "\n",
    "### if there is expected to be a combinatorial explosion the following filter process is applied to ensure completion\n",
    "\n",
    "if combinatorial_explosion == True:\n",
    "    print ('!!!combinatorial_explosion occurs, using flying edge method!!!')\n",
    "\n",
    "    \n",
    "### Collect all unique nodes from non-donor graphs\n",
    "    valid_nodes = []\n",
    "    for edges in filteredcomplete_nodonor_combinations:\n",
    "        for i, j in edges:\n",
    "            if i not in valid_nodes:\n",
    "                valid_nodes.append(i)\n",
    "            if j not in valid_nodes:\n",
    "                valid_nodes.append(j)\n",
    "    print (valid_nodes)\n",
    "\n",
    "    \n",
    "### Filter edges_donor based on donor labeling and node validity\n",
    "    valid_donoredges_donor = []\n",
    "    valid_metaledges_donor = []\n",
    "    \n",
    "    for i, j in edges_donor:\n",
    "        #print (nodes)\n",
    "        if 'Donor' in nodes[i] and 'Donor' in nodes[j]:\n",
    "            if abs(i - j) >= ((len(nodes))//monomers):\n",
    "                valid_donoredges_donor.append((i, j))\n",
    "            #print((nodes[i], nodes[j]))\n",
    "        elif i in valid_nodes or j in valid_nodes:\n",
    "            for n in bonds_not_constructed:\n",
    "                if nodes[i] not in n and nodes[j] not in n:\n",
    "                    valid_metaledges_donor.append((i, j))\n",
    "                else:\n",
    "                    print (i,j)\n",
    "            #print((i, j))\n",
    "    print (valid_metaledges_donor)\n",
    "    print (valid_donoredges_donor)\n",
    "\n",
    "    \n",
    "### generating all combinations of M-donor edges\n",
    "    \n",
    "    temp_all_possible_combinations = []\n",
    "\n",
    "    for length in range(1,int(monomers*donor_M_connection)):\n",
    "        print(length)\n",
    "        combinations = list(itertools.combinations(list(range(len(valid_metaledges_donor))), length))\n",
    "        temp_all_possible_combinations = temp_all_possible_combinations + combinations\n",
    "    print(len(temp_all_possible_combinations))\n",
    "    tempall_possible_graphs = []\n",
    "    for combinations in temp_all_possible_combinations:\n",
    "        edges = []\n",
    "        #print (combinations)\n",
    "        for edge in combinations:\n",
    "            #print (edge)\n",
    "            edges.append(valid_metaledges_donor[int(edge)])\n",
    "        tempall_possible_graphs.append(edges)\n",
    "    print(len(tempall_possible_graphs))\n",
    "\n",
    "\n",
    "### filter M-donor edges based on connectivities defined in the .csv file\n",
    "    \n",
    "    def filter_graph(index, graph, connectivity_allowed):\n",
    "        \"\"\"\n",
    "        Check if the graph at the given index meets the specified connectivity criteria.\n",
    "        If not, mark it as 'tbr' (to be removed).\n",
    "        \"\"\"\n",
    "        G = nx.Graph()  # Create a new graph instance for this function\n",
    "        for edge in graph:\n",
    "            G.add_edge(edge[0], edge[1])\n",
    "    \n",
    "        for node in list(G.nodes):\n",
    "            degrees = G.degree(node)\n",
    "            if degrees not in connectivity_allowed[node]:\n",
    "                return 'tbr'\n",
    "        return graph\n",
    "\n",
    "# Parallelize the filtering process\n",
    "    filtered2_combinations = copy.copy(tempall_possible_graphs)\n",
    "    results = Parallel(n_jobs=num_cores)(delayed(filter_graph)(i, graph, connectivity_allowed) for i, graph in enumerate(tempall_possible_graphs))\n",
    "    filtered2_combinations = results\n",
    "    filtered3_combinations = [graph for graph in filtered2_combinations if graph != 'tbr']\n",
    "    print(len(filtered3_combinations))\n",
    "\n",
    "    \n",
    "### this restricts the number of donor metal interactions as specified by the user \n",
    "    \n",
    "    import copy\n",
    "    filtered4_combinations = copy.copy(filtered3_combinations)\n",
    "\n",
    "\n",
    "    for i in range(0, len(filtered3_combinations)):\n",
    "        G.clear()\n",
    "        #print(i)\n",
    "        for edge in filtered3_combinations[i]:            \n",
    "            G.add_edge(edge[0],edge[1])\n",
    "        \n",
    "        for node in list(G.nodes):\n",
    "            donor_metal = nx.degree(G, nbunch=node)\n",
    "            if donor_metal > donor_M_connection:\n",
    "                #print('yes')\n",
    "                filtered4_combinations[i] = 'tbr'\n",
    "                break\n",
    "            \n",
    "    filtered5_combinations = []\n",
    "    for edges in filtered4_combinations:\n",
    "        if edges != 'tbr':\n",
    "            filtered5_combinations.append(edges)\n",
    "\n",
    "\n",
    "    print (len(filtered5_combinations))\n",
    "\n",
    "    \n",
    "### filter isomorph graphs \n",
    "    def compute_graph_hash(edges, node_labels):\n",
    "        # Create a local graph instance for this set of edges\n",
    "        local_G = nx.Graph()\n",
    "        local_G.add_nodes_from([(i, {\"label\": label}) for i, label in enumerate(node_labels)])\n",
    "        local_G.add_edges_from(edges)\n",
    "    \n",
    "        # Compute and return the Weisfeiler-Lehman graph hash\n",
    "        return nx.weisfeiler_lehman_graph_hash(local_G, node_attr='label')\n",
    "\n",
    "    node_labels = nodes\n",
    "    # Parallelize the computation of graph hashes\n",
    "    graphs_hashes = Parallel(n_jobs=num_cores)(delayed(compute_graph_hash)(edges, node_labels) for edges in filtered5_combinations)\n",
    "    # Sequential part: filtering unique graphs based on hashes\n",
    "    filtered5_edges = []\n",
    "    filtered5_graphs = []\n",
    "    for i, graph_hash in enumerate(graphs_hashes):\n",
    "        if graph_hash not in filtered5_graphs:\n",
    "            filtered5_edges.append(filtered5_combinations[i])\n",
    "            filtered5_graphs.append(graph_hash)\n",
    "    print(len(filtered5_edges))\n",
    "\n",
    "    \n",
    "### generate all combinations of the donor-donor edges\n",
    "    \n",
    "    donortemp_all_possible_combinations = []\n",
    "    combinations = []\n",
    "    for length in range(1,int(len(valid_donoredges_donor)/monomers)+1):\n",
    "        print(length)\n",
    "        combinations = list(itertools.combinations(list(range(len(valid_donoredges_donor))), length))\n",
    "        donortemp_all_possible_combinations = donortemp_all_possible_combinations + combinations\n",
    "    print(len(donortemp_all_possible_combinations))\n",
    "    donortempall_possible_graphs = []\n",
    "    for combinations in donortemp_all_possible_combinations:\n",
    "        edges = []\n",
    "        #print (combinations)\n",
    "        for edge in combinations:\n",
    "            #print (edge)\n",
    "            edges.append(valid_donoredges_donor[int(edge)])\n",
    "        donortempall_possible_graphs.append(edges)\n",
    "    print(len(donortempall_possible_graphs))\n",
    "\n",
    "    \n",
    "### filter the donor-donor graphs based on connectivity allowed\n",
    "    \n",
    "    def filter_graph(index, graph, connectivity_allowed):\n",
    "        \"\"\"\n",
    "        Check if the graph at the given index meets the specified connectivity criteria.\n",
    "        If not, mark it as 'tbr' (to be removed).\n",
    "        \"\"\"\n",
    "        G = nx.Graph()  # Create a new graph instance for this function\n",
    "        for edge in graph:\n",
    "            G.add_edge(edge[0], edge[1])\n",
    "    \n",
    "        for node in list(G.nodes):\n",
    "            degrees = G.degree(node)\n",
    "            if degrees not in connectivity_allowed[node]:\n",
    "                return 'tbr'\n",
    "        return graph\n",
    "\n",
    "    donorfiltered2_combinations = copy.copy(donortempall_possible_graphs)\n",
    "    results = Parallel(n_jobs=num_cores)(delayed(filter_graph)(i, graph, connectivity_allowed) for i, graph in enumerate(donortempall_possible_graphs))\n",
    "    donorfiltered2_combinations = results\n",
    "    donorfiltered3_combinations = [graph for graph in donorfiltered2_combinations if graph != 'tbr']\n",
    "    print(len(donorfiltered3_combinations))\n",
    "\n",
    "### filter isomorph graphs \n",
    "    \n",
    "    def compute_graph_hash(edges, node_labels):\n",
    "        # Create a local graph instance for this set of edges\n",
    "        local_G = nx.Graph()\n",
    "        local_G.add_nodes_from([(i, {\"label\": label}) for i, label in enumerate(node_labels)])\n",
    "        local_G.add_edges_from(edges)\n",
    "    \n",
    "        # Compute and return the Weisfeiler-Lehman graph hash\n",
    "        return nx.weisfeiler_lehman_graph_hash(local_G, node_attr='label')\n",
    "\n",
    "    node_labels = nodes\n",
    "    # Parallelize the computation of graph hashes\n",
    "    graphs_hashes = Parallel(n_jobs=num_cores)(delayed(compute_graph_hash)(edges, node_labels) for edges in donorfiltered3_combinations)\n",
    "    # Sequential part: filtering unique graphs based on hashes\n",
    "    donorfiltered3_edges = []\n",
    "    donorfiltered3_graphs = []\n",
    "    for i, graph_hash in enumerate(graphs_hashes):\n",
    "        if graph_hash not in donorfiltered3_graphs:\n",
    "            donorfiltered3_edges.append(donorfiltered3_combinations[i])\n",
    "            donorfiltered3_graphs.append(graph_hash)\n",
    "    print(len(donorfiltered3_edges))\n",
    "\n",
    "    \n",
    "### make all combinations of filtered M-donor and donor-donor graphs\n",
    "    \n",
    "    filteredcomplete_donor_combinations = []\n",
    "\n",
    "    for c in filtered5_edges:\n",
    "        filteredcomplete_donor_combinations.append(c)\n",
    "        for cc in donorfiltered3_edges:\n",
    "            edge = cc + c\n",
    "            filteredcomplete_donor_combinations.append(edge)\n",
    "    print (len(filteredcomplete_donor_combinations))    \n",
    "    plot_graphs(filteredcomplete_donor_combinations[300:324])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070ce19c-dddb-40a2-9b27-8ad33e52359c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### if there is not expected to be a combinatorial explosion then the following general filtering procedure will be used\n",
    "### NOTE: there is no gain or loss of the different filtering processes if the code decides there is not a combinatorial explosion using the previous script will not provide any speed up.\n",
    "\n",
    "if not combinatorial_explosion:\n",
    "    print('normal combination')\n",
    "\n",
    "### generate all combination of donor involved edges\n",
    "    \n",
    "    max_index = len(edges_donor)\n",
    "    if donor_as_bridgeing == True:\n",
    "        max_length = int(max_index)\n",
    "    else:\n",
    "        max_length = int(max_index/monomers)\n",
    "\n",
    "    def generate_donor_combinations(length, max_index):\n",
    "        result = list(itertools.combinations(range(max_index), length))\n",
    "        print(f\"[PID {multiprocessing.current_process().pid}] Length {length} done with {len(result)} combinations\")\n",
    "        return result\n",
    "\n",
    "    # Run parallel jobs\n",
    "    results = Parallel(n_jobs=num_cores)(\n",
    "        delayed(generate_donor_combinations)(length, max_index)\n",
    "        for length in range(1, max_length + 1)\n",
    "    )\n",
    "    # Flatten\n",
    "    temp_all_possible_combinations = [item for sublist in results for item in sublist]\n",
    "    print(\"Total combinations:\", len(temp_all_possible_combinations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e899165e-6e72-4b29-a2ef-c19772f0a5c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T11:43:41.201216Z",
     "start_time": "2024-07-09T11:43:24.909153Z"
    }
   },
   "outputs": [],
   "source": [
    "### make graphs of all combinations of donors edges\n",
    "\n",
    "if combinatorial_explosion == False:\n",
    "    print ('normal combination')\n",
    "    tempall_possible_graphs = []\n",
    "    for combinations in temp_all_possible_combinations:\n",
    "        edges = []\n",
    "        #print (combinations)\n",
    "        for edge in combinations:\n",
    "            #print (edge)\n",
    "            edges.append(edges_donor[int(edge)])\n",
    "        tempall_possible_graphs.append(edges)\n",
    "    print(len(tempall_possible_graphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856cc968-d522-4183-ae50-2fe199499258",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T11:44:55.587733Z",
     "start_time": "2024-07-09T11:43:41.326609Z"
    }
   },
   "outputs": [],
   "source": [
    "### filters out any graphs with unwanted edges of the donor containing components\n",
    "\n",
    "if combinatorial_explosion == False:\n",
    "    print ('normal combination')\n",
    "    def filter_graph(index, graph, connectivity_allowed):\n",
    "        \"\"\"\n",
    "        Check if the graph at the given index meets the specified connectivity criteria.\n",
    "        If not, mark it as 'tbr' (to be removed).\n",
    "        \"\"\"\n",
    "        G = nx.Graph()  # Create a new graph instance for this function\n",
    "        for edge in graph:\n",
    "            G.add_edge(edge[0], edge[1])\n",
    "    \n",
    "        for node in list(G.nodes):\n",
    "            degrees = G.degree(node)\n",
    "            if degrees not in connectivity_allowed[node]:\n",
    "                return 'tbr'\n",
    "        return graph\n",
    "\n",
    "    filtered2_combinations = copy.copy(tempall_possible_graphs)\n",
    "    # Parallelize the filtering process\n",
    "    results = Parallel(n_jobs=num_cores)(delayed(filter_graph)(i, graph, connectivity_allowed) for i, graph in enumerate(tempall_possible_graphs))\n",
    "    # Update filtered2_combinations with the results\n",
    "    filtered2_combinations = results\n",
    "    # Filter out the graphs marked as 'tbr'\n",
    "    filtered3_combinations = [graph for graph in filtered2_combinations if graph != 'tbr']\n",
    "\n",
    "    print(len(filtered3_combinations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdbe037-1814-462e-b8fd-e08cbdac30c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T11:46:42.220767Z",
     "start_time": "2024-07-09T11:44:55.760385Z"
    }
   },
   "outputs": [],
   "source": [
    "### this filter restricts the number of donor metal interactions as user specified\n",
    "\n",
    "if combinatorial_explosion == False:\n",
    "    print ('normal combination')\n",
    "    \n",
    "    import copy\n",
    "    filtered4_combinations = copy.copy(filtered3_combinations)\n",
    "\n",
    "\n",
    "    for i in range(0, len(filtered3_combinations)):\n",
    "        G.clear()\n",
    "        #print(i)\n",
    "        for edge in filtered3_combinations[i]:            \n",
    "            G.add_edge(edge[0],edge[1])\n",
    "        \n",
    "        for node in list(G.nodes):\n",
    "            donor_metal = nx.degree(G, nbunch=node)\n",
    "            if donor_metal > donor_M_connection:\n",
    "                #print('yes')\n",
    "                filtered4_combinations[i] = 'tbr'\n",
    "                break\n",
    "            \n",
    "\n",
    "    filtered5_combinations = []\n",
    "    for edges in filtered4_combinations:\n",
    "        if edges != 'tbr':\n",
    "            filtered5_combinations.append(edges)\n",
    "    \n",
    "    filteredcomplete_donor_combinations = filtered5_combinations\n",
    "    print (len(filtered5_combinations))\n",
    "    #plot_graphs(filtered5_combinations[100:110])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104673c5-8c27-4fe5-a8f9-7f8ea4c5715a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T11:46:43.960850Z",
     "start_time": "2024-07-09T11:46:43.955208Z"
    }
   },
   "outputs": [],
   "source": [
    "#*************************************************create all graphs****************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe828791-ff92-4af1-adb4-428843d195f3",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-09T11:46:43.958210Z"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "### make all combinations of donor and non-donor involved edges\n",
    "\n",
    "\n",
    "edges = []\n",
    "\n",
    "for c in filteredcomplete_nodonor_combinations:\n",
    "    for cc in filteredcomplete_donor_combinations:\n",
    "        edge = cc + c\n",
    "        edges.append(edge)\n",
    "print (len(edges))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8554b12c-9399-497b-b9e7-4b9c317c0180",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "### this filters any graphs that are incorrectly charged\n",
    "\n",
    "filtered6_edges = []\n",
    "if Donorligand_charged == True:\n",
    "    #filtered4_nodonor_combinations = filtered3_nodonor_combinations  # No need to filter further\n",
    "    for i in range(0, len(edges)):\n",
    "        Edges = list(G.edges)\n",
    "        G.clear()\n",
    "        G.remove_edges_from(Edges)\n",
    "        for edge in edges[i]:            \n",
    "            G.add_edge(edge[0],edge[1])\n",
    "        charge = 0\n",
    "    \n",
    "        #print(list(G.edges))\n",
    "\n",
    "        Donornodes = []\n",
    "        Donor_charges = 0\n",
    "        for node in list(G.nodes):\n",
    "            if 'Donor' in nodes[node]:\n",
    "                Donornodes.append(node)\n",
    "            else: \n",
    "                charge = charge + charges[node]\n",
    "                \n",
    "        for c in range(monomers):\n",
    "            start = c * ((len(nodes))//monomers)\n",
    "            end = start + ((len(nodes))//monomers)\n",
    "            current_range = range(start, end)\n",
    "            if any(d in current_range for d in Donornodes):\n",
    "                Donor_charges += donor_total_charge\n",
    "                continue\n",
    "        #print ('total donor charge:')\n",
    "        #print (Donor_charges)\n",
    "        charge = charge + Donor_charges\n",
    "        #print (charge)\n",
    "        if charge == charge_specificed:\n",
    "            filtered6_edges.append(edges[i])\n",
    "            #print ('yes')\n",
    "    \n",
    "    print('donor charge filter')\n",
    "    print(len(filtered6_edges))\n",
    "else:\n",
    "    print('no donor charge filter')\n",
    "    for i in range(0, len(edges)):\n",
    "        Edges = list(G.edges)\n",
    "        G.clear()\n",
    "        G.remove_edges_from(Edges)\n",
    "        for edge in edges[i]:            \n",
    "            G.add_edge(edge[0],edge[1])\n",
    "        charge = 0\n",
    "    \n",
    "        #print(list(G.nodes))\n",
    "\n",
    "    \n",
    "        for node in list(G.nodes):\n",
    "            charge = charge + charges[node]\n",
    "            #print (node, charges[node])\n",
    "        #print (charge)\n",
    "        if charge == charge_specificed:\n",
    "            filtered6_edges.append(edges[i])\n",
    "            #print ('yes')\n",
    "    print(len(filtered6_edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5274648b-2b50-482c-85c5-7566162c7537",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_graphs(filtered6_edges[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d59121-870e-4e68-9fa2-63ac8369d6a8",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "### this filters out any graphs with not allowed edges\n",
    "\n",
    "G = nx.Graph(label=\"x\")\n",
    "for i in range(0, len(nodes)):\n",
    "    #print(i)\n",
    "    G.add_nodes_from([\n",
    "        (i, {\"label\": nodes[i]}),\n",
    "        ])\n",
    "\n",
    "a = nx.get_node_attributes(G,'label')\n",
    "print(a)\n",
    "\n",
    "import copy\n",
    "filtered_edges = copy.copy(filtered6_edges)\n",
    "\n",
    "\n",
    "for i in range(0, len(filtered6_edges)):\n",
    "    Edges = list(G.edges)\n",
    "    G.remove_edges_from(Edges)\n",
    "    for edge in filtered6_edges[i]:            \n",
    "        G.add_edge(edge[0],edge[1])\n",
    "        if a[edge[0]] == a[edge[1]]:\n",
    "            filtered_edges[i] = 'tbr'\n",
    "            #print ('1111')\n",
    "            break\n",
    "    for node, attribute in a.items():\n",
    "        degrees = nx.degree(G, nbunch=node)\n",
    "        if degrees not in connectivity_allowed[node]:\n",
    "            #print (degrees, connectivity_allowed[node])\n",
    "            filtered_edges[i] = 'tbr'\n",
    "            #print ('2222')\n",
    "            break\n",
    "            \n",
    "\n",
    "filtered2_edges = []\n",
    "for edge in filtered_edges:\n",
    "    if edge != 'tbr':\n",
    "        filtered2_edges.append(edge)\n",
    "print (len(filtered2_edges))\n",
    "\n",
    "\n",
    "# here we add again all the filter graphs for all graphs without donor atoms\n",
    "for c in filteredcomplete_nodonor_combinations:\n",
    "    filtered2_edges.append(c)\n",
    "\n",
    "\n",
    "\n",
    "print (len(filtered2_edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04649e0-ca43-4551-aa8d-ba28896e1198",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_graphs (filtered2_edges[0:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eba7b1b-87b1-4a9d-88a8-6ef6f8bf3156",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "### this filters out the isomorphic graphs roughly before split nonconnected subgraphs\n",
    "\n",
    "\n",
    "def compute_graph_hash(edges, node_labels):\n",
    "\n",
    "    local_G = nx.Graph()\n",
    "    local_G.add_nodes_from([(i, {\"label\": label}) for i, label in enumerate(node_labels)])\n",
    "    local_G.add_edges_from(edges)\n",
    "    \n",
    "    # Compute and return the Weisfeiler-Lehman graph hash\n",
    "    return nx.weisfeiler_lehman_graph_hash(local_G, node_attr='label')\n",
    "# Assuming 'nodes' and 'filtered2_edges' are defined\n",
    "node_labels = nodes\n",
    "\n",
    "# Parallelize the computation of graph hashes\n",
    "graphs_hashes = Parallel(n_jobs=num_cores)(delayed(compute_graph_hash)(edges, node_labels) for edges in filtered2_edges)\n",
    "\n",
    "# Sequential part: filtering unique graphs based on hashes\n",
    "filtered5_edges = []\n",
    "filtered5_graphs = []\n",
    "for i, graph_hash in enumerate(graphs_hashes):\n",
    "    if graph_hash not in filtered5_graphs:\n",
    "        filtered5_edges.append(filtered2_edges[i])\n",
    "        filtered5_graphs.append(graph_hash)\n",
    "\n",
    "print(len(filtered5_edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118d875d-9f9e-4a45-86ee-700db31556a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs (filtered5_edges[3000:3001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3024ab91-99e2-4567-8571-f9e88ba94abf",
   "metadata": {
    "is_executing": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### this filters and splits the unconnected graphs into sub graphs, keeping the one with the max connectivity\n",
    "\n",
    "filtered3_edges = copy.copy(filtered5_edges)\n",
    "\n",
    "for i in range(0, len(filtered5_edges)):\n",
    "    Edges = list(G.edges)\n",
    "    #print(i)\n",
    "    G.remove_edges_from(Edges)\n",
    "    to_remove = []\n",
    "    for edge in filtered5_edges[i]:            \n",
    "        G.add_edge(edge[0],edge[1])\n",
    "\n",
    "    if donor_as_bridgeing: \n",
    "        #print('additional filter for bridging multidentate ligand turned on')\n",
    "        Nodees = list(G.nodes)             ####here we add the temperory edges of the donors to ensure they are not filtered out\n",
    "        temp_donor = []\n",
    "        for n in Nodees:\n",
    "            if 'Donor' in nodes[n]:\n",
    "                temp_donor.append(n)            \n",
    "        for c in range(monomers):\n",
    "            #print (c)\n",
    "            start = c * ((len(nodes))//monomers)\n",
    "            end = start + ((len(nodes))//monomers)\n",
    "            current_range = range(start, end)\n",
    "            matching = [t for t in temp_donor if t in current_range]\n",
    "            if len(matching) > 1:\n",
    "                #print ('number of nodes', len(matching))\n",
    "                unique_pairs = list(itertools.combinations(matching, 2))\n",
    "                #print ('unique_pairs', unique_pairs)\n",
    "                for pair in unique_pairs:\n",
    "                    G.add_edge(pair[0],pair[1])\n",
    "                    to_remove.append((pair[0],pair[1]))\n",
    "                    #print ('to-remove', to_remove)\"\"\"\n",
    "                \n",
    "    if nx.is_connected(G)!=True:\n",
    "        component=list(max(nx.connected_components(G)))\n",
    "        for N in list(G.nodes):\n",
    "            if N not in component:\n",
    "                G.remove_node(N)\n",
    "        #print(list(G.edges))\n",
    "        if to_remove != []:\n",
    "            G.remove_edges_from(to_remove)\n",
    "        edgee = list(G.edges)\n",
    "        nodee = list(G.nodes)\n",
    "        #print(edgee)\n",
    "        filtered3_edges[i] = edgee\n",
    "\n",
    "\n",
    "filtered4_edges = []\n",
    "\n",
    "for edge in filtered3_edges:\n",
    "    #print (edge)\n",
    "    #print ('checking')\n",
    "    if edge not in filtered4_edges:\n",
    "        if len(edge) != 0:\n",
    "            #print ('yes edge included')\n",
    "            filtered4_edges.append(edge)\n",
    "            #print (filtered4_edges)\n",
    "print (len(filtered4_edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbe5d18-0cff-4fdf-ac10-540ba957d0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs (filtered4_edges[1000:1020])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6231321f-b0c4-4073-9e02-b471466c8501",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "### this filters out the isomorphic graphs again\n",
    "graphs = []\n",
    "filtered7_edges = []\n",
    "filtered7_graphs = []\n",
    "\n",
    "        \n",
    "G = nx.Graph(label=\"x\")\n",
    "for i in range(0, len(nodes)):\n",
    "    #print(i)\n",
    "    \n",
    "    G.add_nodes_from([\n",
    "        (i, {\"label\": nodes[i]}),\n",
    "        ])\n",
    "\n",
    "\n",
    "for i in range(0, len(filtered4_edges)):\n",
    "    Edges = list(G.edges)\n",
    "    G.remove_edges_from(Edges)\n",
    "    \n",
    "    for edge in filtered4_edges[i]:            \n",
    "        G.add_edge(edge[0],edge[1])\n",
    "\n",
    "    \n",
    "    graphs.append(nx.weisfeiler_lehman_graph_hash(G, node_attr='label'))\n",
    "\n",
    "\n",
    "\n",
    "for i in range(0, len(graphs)):\n",
    "    if graphs[i] not in filtered7_graphs:\n",
    "        filtered7_edges.append(filtered4_edges[i])\n",
    "        filtered7_graphs.append(graphs[i])\n",
    "\n",
    "print (len(filtered7_edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac8e692-03d3-4431-b8c7-32652ad820b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(filtered7_edges[160:165])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78777d3d-6b22-43bc-82e0-1a3e1c117101",
   "metadata": {
    "is_executing": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### this filters any graphs that are charged again after split is needed\n",
    "filtered8_edges = []\n",
    "\n",
    "if Donorligand_charged == True:\n",
    "\n",
    "    for i in range(0, len(filtered7_edges)):\n",
    "        Edges = list(G.edges)\n",
    "        G.clear()\n",
    "        G.remove_edges_from(Edges)\n",
    "        for edge in filtered7_edges[i]:            \n",
    "            G.add_edge(edge[0],edge[1])\n",
    "    \n",
    "        #print(list(G.nodes))\n",
    "\n",
    "        Donornodes = []\n",
    "        Donor_charges = 0\n",
    "        charge = 0\n",
    "        for node in list(G.nodes):\n",
    "            if 'Donor' in nodes[node]:\n",
    "                #print ('yes')\n",
    "                Donornodes.append(node)\n",
    "            else: \n",
    "                charge = charge + charges[node]\n",
    "                #print ('non donor charges')\n",
    "                #print (charge)\n",
    "                \n",
    "        for c in range(monomers):\n",
    "            start = c * ((len(nodes))//monomers)\n",
    "            end = start + ((len(nodes))//monomers)\n",
    "            current_range = range(start, end)\n",
    "            #print (current_range)\n",
    "            if any(d in current_range for d in Donornodes):\n",
    "                Donor_charges += donor_total_charge\n",
    "                #print ('yes', donor_total_charge )\n",
    "                continue\n",
    "        #print ('total donor charge:')\n",
    "        #print (Donor_charges)\n",
    "        charge = charge + Donor_charges\n",
    "        #print ('total donor charge:')\n",
    "        #print (Donor_charges)\n",
    "        if charge == charge_specificed:\n",
    "            filtered8_edges.append(filtered7_edges[i])\n",
    "            #print ('total charge = 0')\n",
    "    \n",
    "    print('donor charge filter')\n",
    "    print(len(filtered8_edges))\n",
    "else:\n",
    "    print('no donor charge filter')\n",
    "    for i in range(0, len(filtered7_edges)):\n",
    "        Edges = list(G.edges)\n",
    "        G.clear()\n",
    "        G.remove_edges_from(Edges)\n",
    "        for edge in filtered7_edges[i]:            \n",
    "            G.add_edge(edge[0],edge[1])\n",
    "        charge = 0\n",
    "    \n",
    "        #print(list(G.nodes))\n",
    "\n",
    "    \n",
    "        for node in list(G.nodes):\n",
    "            charge = charge + charges[node]\n",
    "            #print (node, charges[node])\n",
    "        #print (charge)\n",
    "        if charge == charge_specificed:\n",
    "            filtered8_edges.append(filtered7_edges[i])\n",
    "            #print ('yes')\n",
    "    print(len(filtered8_edges))\n",
    "complete_filtered_edges = filtered8_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dd5164-f1bf-4fda-b4f7-72334c5977cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### filter user specificed controll of atom nums:\n",
    "\n",
    "filtered9_edges = []\n",
    "filtered_edges = copy.copy(filtered8_edges)\n",
    "\n",
    "if user_specified_atomcount_controll:\n",
    "    print ('now filter according to user defined atom_num')\n",
    "    for i in range(0, len(filtered8_edges)):\n",
    "        Edges = list(G.edges)\n",
    "        G.clear()\n",
    "        G.remove_edges_from(Edges)\n",
    "        for edge in filtered8_edges[i]:            \n",
    "            G.add_edge(edge[0],edge[1])\n",
    "        charge = 0\n",
    "    \n",
    "        #print(list(G.nodes))\n",
    "\n",
    "        for atom, num in user_specified_atomcount_controll:\n",
    "            count = 0\n",
    "            for node in list(G.nodes):\n",
    "                if nodes[node] == atom:\n",
    "                    count+=1\n",
    "            if count != num:\n",
    "                filtered_edges[i] = 'tbr'\n",
    "                break\n",
    "\n",
    "    for edges in filtered_edges:\n",
    "        #print (edges)\n",
    "        if edges != 'tbr':\n",
    "            filtered9_edges.append(edges)\n",
    "            #print (filtered6_nodonor_combinations)\n",
    "        \n",
    "    complete_filtered_edges = filtered9_edges\n",
    "print (len(complete_filtered_edges))\n",
    "#print (filteredcomplete_nodonor_combinations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905785fa-ed22-45a1-8055-9ec677c6b02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### additional trouble shooting function that allow you to check if a certain combination of edge is present in the filtered graphs\n",
    "\n",
    "normalize = lambda e: tuple(sorted(e))\n",
    "target_pairs = {normalize(e) for e in [(0,2), (0,3), (0,4), (5,7), (5,8), (5,9), (0,9), (5,4)]}\n",
    "\n",
    "# Search for target set in valid combinations and get index\n",
    "for i, combo in enumerate(complete_filtered_edges):  # or 'edges' if that's your list\n",
    "    normalized_combo = {normalize(e) for e in combo}\n",
    "    if target_pairs.issubset(normalized_combo):\n",
    "        print(\"✅ Found matching combination!\")\n",
    "        print(f\"   ➤ Index in list: {i}\")\n",
    "        print(f\"   ➤ Combo edges: {combo}\")\n",
    "        # plot_graphs([combo])\n",
    "        break\n",
    "else:\n",
    "    print(\"❌ No matching combination found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bf9079-8dd0-42b0-96fe-ed76b20c17fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_graphs(complete_filtered_edges[140:145])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a970ac2-a98d-4745-a541-d0e2d2d0e4c2",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "### constructure all filtered combintaion of all edges into graphs\n",
    "\n",
    "GRAPHS = []\n",
    "\n",
    "G = nx.Graph(label=\"x\")\n",
    "\n",
    "for i in range(len(complete_filtered_edges)):\n",
    "    G.clear()\n",
    "    \n",
    "    for edge in complete_filtered_edges[i]:\n",
    "        G.add_edge(edge[0],edge[1])\n",
    "        \n",
    "    for node in list(G.nodes):\n",
    "        G.add_nodes_from([\n",
    "            (node, {'label': nodes[node]}),\n",
    "            ])\n",
    "    \n",
    "    Gtemp = G.copy()\n",
    "    GRAPHS.append(Gtemp)\n",
    "print(len(GRAPHS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb0f269-d42b-49ad-9213-079759efb564",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "#**********************************************creating the 2D connection table**********************************#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1dad0a-aae1-4122-ab07-efb02c39d807",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "### function to get the num of atoms in ct\n",
    "\n",
    "\n",
    "def get_atoms_num(filename):\n",
    "    with open (filename + '.ct', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        someinfo = lines[1].split()[0]\n",
    "        return someinfo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261e2e16-aff1-403c-a90e-5b35367f3b62",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "### function to get the atoms and coordinates connections in ct, returns a list of lists, each list = [x, y, atom type]\n",
    "\n",
    "\n",
    "def get_coords(filename):\n",
    "    with open (filename + '.ct', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        atoms_num = int(lines[1].split()[0])\n",
    "        coords = []\n",
    "        coord = []\n",
    "        for line in lines[2 : 2 + atoms_num]:\n",
    "            x = line.split()\n",
    "            coo = [x[0], x[1], x[3]]\n",
    "            coords.append(coo)\n",
    "    return coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c93206-4ce3-46a8-8432-576db6b9c5e9",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "### function to get the connectivities in ct, returns a list of lists, each list = [atom1, atom2, bond type, bond type]\n",
    "\n",
    "\n",
    "def get_connect(filename):\n",
    "    with open (filename + '.ct', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        atoms_num = int(lines[1].split()[0])\n",
    "        connect_num = int(lines[1].split()[1])\n",
    "        connect = []\n",
    "\n",
    "        for line in lines[2 + atoms_num : 2 + atoms_num + connect_num ]:\n",
    "            x = line.split()\n",
    "            coo = [x[0], x[1], x[2], x[3]]\n",
    "            connect.append(coo)\n",
    "    return (connect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14822b00-8843-409b-8170-913f6efd6367",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "###function to get the num of connections in ct \n",
    "\n",
    "\n",
    "def get_connect_num(filename):\n",
    "    with open (filename + '.ct', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        someinfo = lines[1].split()[1]\n",
    "        return someinfo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee03fbad-868c-4dee-9942-58a43e3e6c7b",
   "metadata": {
    "is_executing": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### converting all graphs into 2D connection tables including all atoms and predefined bonds\n",
    "\n",
    "\n",
    "i=0\n",
    "\n",
    "for G in GRAPHS:\n",
    "\n",
    "    open('speciation' + str(i) + '.ct', '+a').write('speciation' + str(i) + '.ct\\n' + '0 0')\n",
    "    connects = []\n",
    "    \n",
    "    for node in sorted(list(G.nodes)):\n",
    "        #print(node)\n",
    "        if 'Donor' not in nodes[node]:\n",
    "            filename = nodes[node]\n",
    "            #print (filename)\n",
    "        elif 'Donor' in nodes[node]:\n",
    "            filename = nodes[node]\n",
    "            #print (filename)\n",
    "            if int(filename[-1]) == 1:\n",
    "                #print ('nod1')\n",
    "                filename = nodes[node].replace('Donor', '')[:-2]\n",
    "\n",
    "            elif int(filename[-1]) == 2:\n",
    "                filename = nodes[node].replace('Donor', '')[:-2]\n",
    "                #print (filename)\n",
    "                #print (node, str(int(node)-1))\n",
    "                if int(node)-1 in list(G.nodes):\n",
    "                    with open('speciation' + str(i) + '.ct', 'r') as f:\n",
    "                        lines = f.read()\n",
    "                        lines = lines.replace('Nod2', str(node))\n",
    "                        #print (lines)\n",
    "                    with open('speciation' + str(i) + '.ct', 'w') as f:    \n",
    "                        f.write(lines)\n",
    "                        #print('skip')\n",
    "                    continue\n",
    "                \n",
    "            elif int(filename[-1]) == 3:\n",
    "                filename = nodes[node].replace('Donor', '')[:-2]\n",
    "                #print (node, str(int(node)-1), str(int(node)-3))\n",
    "                #print (filename)\n",
    "                if int(node)-1 in list(G.nodes) or int(node)-2 in list(G.nodes):\n",
    "                    with open('speciation' + str(i) + '.ct', 'r') as f:\n",
    "                        lines = f.read()\n",
    "                        lines = lines.replace('Nod3', str(node))\n",
    "                        #print (lines)\n",
    "                    with open('speciation' + str(i) + '.ct', 'w') as f:\n",
    "                        f.write(lines)\n",
    "                        #print('skip')\n",
    "                    continue\n",
    "        #print (filename)\n",
    "        old_atom_num = int(get_atoms_num('speciation' + str(i)))      ######\n",
    "        new_atom_num = int(get_atoms_num(filename)) + old_atom_num    ######\n",
    "        old_atom_connect = int(get_connect_num('speciation' + str(i)))######\n",
    "        new_atom_connect = int(get_connect_num(filename)) + old_atom_connect   ######\n",
    "\n",
    "        with open('speciation' + str(i) + '.ct', 'r') as f:           ######\n",
    "            lines = f.readlines()                                     ######\n",
    "            lines[1] = str(new_atom_num) + ' ' + str(new_atom_connect) +'\\n'   ###### update total number of atom and bonds\n",
    "        with open('speciation' + str(i) + '.ct', 'w') as f:           ######\n",
    "            f.writelines(lines)                                       ######\n",
    "            coords = get_coords(filename)                             ######\n",
    "            for coord in coords:                                      ######\n",
    "                coord[0] = str(float(coord[0]) + 10 * node)           ###### writing the new atoms from the node to the file\n",
    "                coord = [coo.replace('Node', str(node)) for coo in coord]       ###### replacing the donor atom with node number per each cycle\n",
    "                #print (coord)\n",
    "                if 'Donor' in nodes[node]:\n",
    "                    temp = nodes[node]\n",
    "                    if int(temp[-1]) == 1:\n",
    "                        coord = [coo.replace('Nod1', str(node)) for coo in coord] \n",
    "                    elif int(temp[-1]) == 2:\n",
    "                        coord = [coo.replace('Nod2', str(node)) for coo in coord]\n",
    "                    elif int(temp[-1]) == 3:\n",
    "                        coord = [coo.replace('Nod3', str(node)) for coo in coord]\n",
    "                f.writelines(coord[0] + '    ' + coord[1] + '     ' + '0.00000' + '     ' + coord[2] +'\\n') ######\n",
    "            ligand_connections = get_connect(filename)                ######\n",
    "\n",
    "        CONNCT = []                                                   ###### now get all the connections and store them in connects\n",
    "        for ligand_connection in ligand_connections:                  ######\n",
    "            atom1 = int(ligand_connection[0]) + int(old_atom_num)     ######\n",
    "            atom2 = int(ligand_connection[1]) + int(old_atom_num)     ######\n",
    "            bonda = ligand_connection[2]                              ######\n",
    "            bondb = ligand_connection[3]                              ######\n",
    "            CONNCT = [atom1, atom2, bonda, bondb]                     ######\n",
    "            connects.append(CONNCT)                                   ######\n",
    "    \n",
    "    with open('speciation' + str(i) + '.ct', 'r') as f: \n",
    "        lines = f.read()\n",
    "        for line in lines:\n",
    "            #match = re.search(r'Nod(\\d)', lines)\n",
    "            updated_lines = re.sub(\n",
    "                r'Nod(\\d)', \n",
    "                lambda m: donor_nodx_atoms[int(m.group(1)) - 1],  # Adjust index if needed\n",
    "                lines\n",
    "            )\n",
    "            #if match:\n",
    "            #    digit = match.group(1)\n",
    "            #    print (digit)\n",
    "            #    lines = re.sub(r'Nod(\\d)', donor_atoms[int(digit)+1], lines)\n",
    "            #    print(lines)\n",
    "    \n",
    "    with open('speciation' + str(i) + '.ct', 'w') as f:    \n",
    "        f.write(updated_lines)    \n",
    "        #print(updated_lines)\n",
    "        \n",
    "    for connect in connects:                                           ######\n",
    "        try:                                                           ######\n",
    "            open('speciation' + str(i) + '.ct', '+a').write(str(connect[0]) + ' ' + str(connect[1]) + '  ' + str(connect[2]) + '  ' + str(connect[3]) + '\\n')\n",
    "        except:                                                        ######\n",
    "            pass                                                       ######\n",
    "\n",
    "    i = i + 1\n",
    "    #print (i)\n",
    "\n",
    "    print (new_atom_num)\n",
    "\n",
    "    \n",
    "#open('speciation' + str(i) + '.ct', '+a').write('speciation' + str(i) + '.ct\\n' + '0 0')        \n",
    "#open('speciation' + str(i) + '.ct', '+a').write(line)        \n",
    "#open('speciation' + str(i) + '.ct', '+a').write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a45eb6b-1bfe-421a-9c14-d0e9c2c8f530",
   "metadata": {
    "is_executing": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### add the newly generated bonds from the graphs to the generated connection tables\n",
    "### important please remove old .ct and start fresh run do not run the code twice it will break\n",
    "\n",
    "i=0\n",
    "\n",
    "for G in GRAPHS:\n",
    "    # and now translating the edges to bonds and write the connection to ct\n",
    "    with open('speciation' + str(i) + '.ct', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        connects = []\n",
    "        for edge in sorted(list(G.edges)):\n",
    "            #print(edge)\n",
    "            connect = []\n",
    "            atom_num_stop_point = int(get_atoms_num('speciation' + str(i)))+2\n",
    "            for line in lines[2:atom_num_stop_point]:\n",
    "                Coords = line.split()\n",
    "                #print(line)\n",
    "                #print(Coords[3])\n",
    "               \n",
    "                if str(Coords[3]) == str(edge[0]):\n",
    "                    atom1 = lines.index(line)-1\n",
    "                    connect.append(atom1)\n",
    "                    #print('yes', atom1)\n",
    "                \n",
    "                if str(Coords[3]) == str(edge[1]):\n",
    "                    atom2 = lines.index(line)-1\n",
    "                    connect.append(atom2)\n",
    "                    #print('yes', atom2)\n",
    "                #print(connect)\n",
    "            connects.append(connect) #connects for all edges in a graph\n",
    "            #print(connects)\n",
    "  \n",
    "        for connect in connects:\n",
    "            open('speciation' + str(i) + '.ct', '+a').write(str(connect[0]) + ' ' + str(connect[1]) + '  1  1\\n')\n",
    "            #print (connect)\n",
    "            \n",
    "        \n",
    "        with open('speciation' + str(i) + '.ct', 'r') as f: \n",
    "            lines = f.readlines()\n",
    "            lines[1] = str(get_atoms_num('speciation' + str(i))) + ' ' + str(int(get_connect_num('speciation' + str(i)))+len(connects)) +'\\n'\n",
    "        with open('speciation' + str(i) + '.ct', 'w') as f: \n",
    "            f.writelines(lines)\n",
    "    print(i)\n",
    "                \n",
    "    i = i+1\n",
    "    #print(i)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ddd0cf-ec69-4110-acd5-aad1c3f5230f",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "### now change the node label back to the correct elemental lable\n",
    "\n",
    "\n",
    "i=0\n",
    "\n",
    "for G in GRAPHS:    \n",
    "    for node in sorted(list(G.nodes)):\n",
    "        atom_type = donor_atoms[node]\n",
    "        \n",
    "        with open('speciation' + str(i) + '.ct', 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            \n",
    "            for line in lines [2:int(get_atoms_num('speciation' + str(i)))+2]:\n",
    "                Coords = line.split()\n",
    "                if str(Coords[3]) == str(node):\n",
    "                    #print('yes')\n",
    "                    lines[lines.index(line)] = coord[0] + '    ' + coord[1] + '     ' + '0.00000' + '     ' + donor_atoms[node] +'\\n'\n",
    "        \n",
    "        with open('speciation' + str(i) + '.ct', 'w') as f: \n",
    "            f.writelines(lines)\n",
    "\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8279933-2185-4c65-9ef7-70266933d3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(GRAPHS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4bc5c3-7410-4420-96fe-092c995fea5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#************************************************* 3D xyz structures ****************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd7b07a-2f4a-4234-bcb3-97c5d8429696",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2D to 3D embedding using openbabel, with additional filters\n",
    "\n",
    "\n",
    "non_H_short_bonds = []\n",
    "all_atom_short_bonds = []\n",
    "malformed_xyz_files = []\n",
    "\n",
    "def is_valid_xyz(filename):\n",
    "    \"\"\"\n",
    "    Check if the XYZ file contains valid float coordinates in the expected format.\n",
    "    Also ensures no NaN values are present.\n",
    "    Returns True if valid, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        assert len(lines) >= 3  # Must have header + at least 1 atom line\n",
    "\n",
    "        for line in lines[2:]:\n",
    "            parts = line.split()\n",
    "            if len(parts) < 4:\n",
    "                return False\n",
    "\n",
    "            x = float(parts[1])\n",
    "            y = float(parts[2])\n",
    "            z = float(parts[3])\n",
    "\n",
    "            # Check for NaN values\n",
    "            if any(math.isnan(coord) for coord in (x, y, z)):\n",
    "                return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    except Exception:\n",
    "        return False\n",
    "        \n",
    "\n",
    "def parse_xyz(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()[2:]\n",
    "    atoms = []\n",
    "    coords = []\n",
    "    for line in lines:\n",
    "        parts = line.split()\n",
    "        atoms.append(parts[0])\n",
    "        coords.append([float(x) for x in parts[1:4]])\n",
    "    return atoms, np.array(coords)\n",
    "\n",
    "def passes_non_H_threshold(atoms, coords, threshold=1.6): ### change here if needed\n",
    "    for i in range(len(atoms)):\n",
    "        if atoms[i] not in center:\n",
    "            continue\n",
    "        for j in range(i + 1, len(atoms)):\n",
    "            if atoms[j] == 'H':\n",
    "                continue\n",
    "            distance = np.linalg.norm(coords[i] - coords[j])\n",
    "            if distance <= threshold:\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "def passes_all_thresholds(atoms, coords, non_H_threshold=1.6, all_atom_threshold=0.9): ## change here if needed\n",
    "    for i in range(len(atoms)):\n",
    "        for j in range(i + 1, len(atoms)):\n",
    "            distance = np.linalg.norm(coords[i] - coords[j])\n",
    "            if distance <= all_atom_threshold:\n",
    "                return False\n",
    "            if atoms[i] in center and atoms[j] != 'H' and distance <= non_H_threshold:\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "def regenerate_structure(index):\n",
    "    inputfile = f\"speciation{index}.ct\"\n",
    "    outputfile = f\"speciation{index}.xyz\"\n",
    "    final_atoms, final_coords = None, None\n",
    "\n",
    "    # First tier: up to 50 attempts for non-H > 1.6 Å\n",
    "    for attempt in range(1, 51):\n",
    "        os.system(f\"obabel {inputfile} -O {outputfile} --gen3d --best > /dev/null 2>&1\")\n",
    "        \n",
    "        if not is_valid_xyz(outputfile):\n",
    "            #malformed_xyz_files.append((index, attempt))\n",
    "            #print(f\"[WARNING] Malformed .xyz file at attempt {attempt} for index {index}. Skipping.\")\n",
    "            non_H_short_bonds.append(index)\n",
    "            return [f\"{index}: Failed non-H distance check (<1.6 Å) after 50 attempts. Saved last geometry.\"]\n",
    "\n",
    "        atoms, coords = parse_xyz(outputfile)\n",
    "        final_atoms, final_coords = atoms, coords\n",
    "\n",
    "        if passes_non_H_threshold(atoms, coords):\n",
    "            break\n",
    "    else:\n",
    "        non_H_short_bonds.append(index)\n",
    "        return [f\"{index}: Failed non-H distance check (<1.6 Å) after 50 attempts. Saved last geometry.\"]\n",
    "\n",
    "    # Second tier: up to 200 attempts for all atoms > 0.9 Å and non-H > 1.6 Å\n",
    "    for attempt in range(51, 201):\n",
    "        if passes_all_thresholds(final_atoms, final_coords):\n",
    "            return [f\"{index}: Valid geometry found after {attempt} attempts.\"]\n",
    "        os.system(f\"obabel {inputfile} -O {outputfile} --gen3d --best > /dev/null 2>&1\")\n",
    "\n",
    "        if not is_valid_xyz(outputfile):\n",
    "            malformed_xyz_files.append((index, attempt))\n",
    "            print(f\"[WARNING] Malformed .xyz file at attempt {attempt} for index {index}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        final_atoms, final_coords = parse_xyz(outputfile)\n",
    "\n",
    "    if not passes_all_thresholds(final_atoms, final_coords):\n",
    "        all_atom_short_bonds.append(index)\n",
    "        return [f\"{index}: Failed all-atom distance check (<0.9 Å or non-H <1.6 Å) after 200 attempts. Saved last geometry.\"]\n",
    "    else:\n",
    "        return [f\"{index}: Valid geometry found within 200 attempts.\"]\n",
    "\n",
    "# Run in parallel\n",
    "results = Parallel(n_jobs=num_cores)(delayed(regenerate_structure)(i) for i in range(len(GRAPHS)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea94bd6-0bc7-4812-8cfd-cb216491f8e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### save all the results into a logfile\n",
    "\n",
    "# Append geometry validation results to logfile.rtf\n",
    "with open('logfile.rtf', \"a\") as f:\n",
    "    for log in results:\n",
    "        for line in log:\n",
    "            print(line)\n",
    "            f.write(line + \"\\n\")\n",
    "\n",
    "# Detect indices that failed the all-atom threshold\n",
    "check_needed = []\n",
    "\n",
    "with open('logfile.rtf', \"r\") as f:\n",
    "    for line in f:\n",
    "        if \"Failed all-atom distance check\" in line:\n",
    "            try:\n",
    "                index = int(line.split(\":\")[0].strip())\n",
    "                check_needed.append(index)\n",
    "            except ValueError:\n",
    "                continue  # Ignore malformed lines\n",
    "\n",
    "# Append the \"check_needed\" indices to the log\n",
    "with open(\"logfile.rtf\", \"a\") as f:\n",
    "    f.write(\"Entries that need checking: \" + \", \".join(map(str, check_needed)) + \"\\n\")\n",
    "\n",
    "# NEW: Append malformed .xyz entries to the log\n",
    "with open(\"logfile.rtf\", \"a\") as f:\n",
    "    f.write(\"\\nMalformed .xyz files encountered:\\n\")\n",
    "    for idx, attempt in malformed_xyz_files:\n",
    "        log_entry = f\" - Index {idx}, attempt {attempt}\"\n",
    "        print(log_entry)\n",
    "        f.write(log_entry + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ace7845-b268-45fa-8fb2-70869ead5724",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### check the fails due to H-H short distances which could be easily fixed \n",
    "\n",
    "logfile_path = \"logfile.rtf\"\n",
    "failed_indices = []\n",
    "\n",
    "#Read logfile and extract failed indices\n",
    "with open(logfile_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        if \"Failed non-H distance check (<1.6 Å) after 50 attempts. Saved last geometry.\" in line:\n",
    "            try:\n",
    "                index = int(line.split(\":\")[0].strip())\n",
    "                failed_indices.append(index)\n",
    "            except ValueError:\n",
    "                continue  # skip malformed lines\n",
    "\n",
    "#Rename the corresponding .xyz files\n",
    "for index in failed_indices:\n",
    "    original_name = f\"speciation{index}.xyz\"\n",
    "    new_name = f\"speciation{index}_nonH_failed.xyz\"\n",
    "\n",
    "    if os.path.exists(original_name):\n",
    "        os.rename(original_name, new_name)\n",
    "        print(f\"Renamed {original_name} → {new_name}\")\n",
    "    else:\n",
    "        print(f\"[WARNING] File not found: {original_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64fd5f0-741c-4152-a10c-f6936ac3ac78",
   "metadata": {
    "is_executing": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### additional functions to fix the incorrectly place H atoms, if the H-H distances are too short\n",
    "### fix all the files with H-H distance issue until all are fixed\n",
    "\n",
    "\n",
    "num_cores = joblib.cpu_count()\n",
    "print(num_cores)\n",
    "\n",
    "def passes_HH_thresholds(atoms, coords, HH_threshold=0.9):\n",
    "    close_pairs = []\n",
    "\n",
    "    for i in range(len(atoms)):\n",
    "        if atoms[i] != 'H':\n",
    "            continue\n",
    "        for j in range(i + 1, len(atoms)):\n",
    "            if atoms[j] != 'H':\n",
    "                continue\n",
    "            distance = np.linalg.norm(coords[i] - coords[j])\n",
    "            if distance <= HH_threshold:\n",
    "                close_pairs.append((i, j))\n",
    "\n",
    "    if close_pairs:\n",
    "        print(close_pairs)\n",
    "        return False, close_pairs\n",
    "    else:\n",
    "        return True, []\n",
    "\n",
    "\n",
    "def find_nearest_carbon(atoms, coords, h_index):\n",
    "    \"\"\"Find the index of the carbon atom nearest to the specified hydrogen atom.\"\"\"\n",
    "    h_coord = coords[h_index]\n",
    "    min_dist = float('inf')\n",
    "    nearest_c_index = None\n",
    "\n",
    "    for i, atom in enumerate(atoms):\n",
    "        if atom == 'C':\n",
    "            dist = np.linalg.norm(h_coord - coords[i])\n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                nearest_c_index = i\n",
    "    return nearest_c_index\n",
    "\n",
    "\n",
    "def rotation_matrix_around_axis(axis, theta):\n",
    "    \"\"\"Return rotation matrix for rotation about given axis by angle theta (radians).\"\"\"\n",
    "    axis = axis / np.linalg.norm(axis)\n",
    "    a = np.cos(theta / 2)\n",
    "    b, c, d = -axis * np.sin(theta / 2)\n",
    "    return np.array([\n",
    "        [a*a + b*b - c*c - d*d,     2*(b*c - a*d),     2*(b*d + a*c)],\n",
    "        [2*(b*c + a*d),     a*a + c*c - b*b - d*d,     2*(c*d - a*b)],\n",
    "        [2*(b*d - a*c),     2*(c*d + a*b),     a*a + d*d - b*b - c*c]\n",
    "    ])\n",
    "\n",
    "def rotate_away(atoms, coords, h1_idx, h2_idx, c_idx, threshold=0.7):\n",
    "    \"\"\"\n",
    "    Rotate H1 and H2 in the H1–C–H2 plane away from each other,\n",
    "    keeping the bond lengths fixed and moving only in-plane.\n",
    "    \"\"\"\n",
    "    h1 = coords[h1_idx]\n",
    "    h2 = coords[h2_idx]\n",
    "    c = coords[c_idx]\n",
    "\n",
    "    vec_h1 = h1 - c\n",
    "    vec_h2 = h2 - c\n",
    "\n",
    "    # Define axis perpendicular to H1–C–H2 plane\n",
    "    axis = np.cross(vec_h1, vec_h2)\n",
    "    if np.linalg.norm(axis) == 0:\n",
    "        print('⚠️ Colinear — generating fallback axis in i–C–(0,0,0) plane')\n",
    "        fallback_axis = np.cross(vec_h1, np.array([0.0, 0.0, 1.0]))\n",
    "        if np.linalg.norm(fallback_axis) == 0:\n",
    "            fallback_axis = np.cross(vec_h1, np.array([1.0, 0.0, 0.0]))\n",
    "        axis = fallback_axis / np.linalg.norm(fallback_axis)\n",
    "    else:\n",
    "        axis = axis / np.linalg.norm(axis)\n",
    "\n",
    "    angle_step = np.deg2rad(5)\n",
    "\n",
    "    # Initial vectors\n",
    "    current_vec_h1 = vec_h1.copy()\n",
    "    current_vec_h2 = vec_h2.copy()\n",
    "\n",
    "    rot_pos = rotation_matrix_around_axis(axis, +angle_step)\n",
    "    rot_neg = rotation_matrix_around_axis(axis, -angle_step)\n",
    "\n",
    "    for _ in range(36):  # up to 180 degrees (5° steps)\n",
    "        current_vec_h1 = np.dot(rot_pos, current_vec_h1)\n",
    "        current_vec_h2 = np.dot(rot_neg, current_vec_h2)\n",
    "        new_h1 = c + current_vec_h1\n",
    "        new_h2 = c + current_vec_h2\n",
    "        dist = np.linalg.norm(new_h1 - new_h2)\n",
    "        #print(f\"H–H distance: {dist:.4f}\")\n",
    "        if dist > threshold:\n",
    "            new_coords = coords.copy()\n",
    "            new_coords[h1_idx] = new_h1\n",
    "            new_coords[h2_idx] = new_h2\n",
    "            print('✅ Fixed via rotation')\n",
    "            return new_coords\n",
    "\n",
    "    print('❌ Rotation failed to separate Hs')\n",
    "    return coords  # fallback if no fix\n",
    "\n",
    "\n",
    "\n",
    "def log(msg, logfile=\"logfile.rtf\"):\n",
    "    print(msg)\n",
    "    with open(logfile, \"a\") as f:\n",
    "        f.write(msg + \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"running now the correction for H-H error in 3D\"\"\"\n",
    "\n",
    "\n",
    "remaining = check_needed.copy()\n",
    "max_loops = 10\n",
    "loop_count = 0\n",
    "\n",
    "while remaining and loop_count < max_loops:\n",
    "    log(f\"\\n🔁 Iteration {loop_count + 1}, remaining: {remaining}\\n\")\n",
    "    next_remaining = []\n",
    "\n",
    "    for spec in remaining:\n",
    "        xyz_file = f\"speciation{spec}.xyz\"\n",
    "        output_file = xyz_file\n",
    "\n",
    "        if not os.path.exists(xyz_file):\n",
    "            log(f\"❌ File not found: {xyz_file}\")\n",
    "            continue\n",
    "\n",
    "        atoms, coords = parse_xyz(xyz_file)\n",
    "        passed, pairs = passes_HH_thresholds(atoms, coords)\n",
    "\n",
    "        if not passed:\n",
    "            for i, j in pairs:\n",
    "                c_index = find_nearest_carbon(atoms, coords, i)\n",
    "                new_coords = rotate_away(atoms, coords, i, j, c_index)\n",
    "                coords = new_coords\n",
    "\n",
    "            with open(output_file, \"w\") as f:\n",
    "                f.write(f\"{len(atoms)}\\n\")\n",
    "                f.write(f\"Fixed geometry for {xyz_file}\\n\")\n",
    "                for atom, coord in zip(atoms, coords):\n",
    "                    f.write(f\"{atom} {coord[0]:.5f} {coord[1]:.5f} {coord[2]:.5f}\\n\")\n",
    "\n",
    "            log(f\"🔧 Fixed and overwrote: {xyz_file}\")\n",
    "            next_remaining.append(spec)\n",
    "        else:\n",
    "            log(f\"✅ {xyz_file} passed HH threshold check.\")\n",
    "\n",
    "    remaining = next_remaining\n",
    "    loop_count += 1\n",
    "\n",
    "if remaining:\n",
    "    log(f\"\\n⚠️ Could not fix the following after {max_loops} attempts: {remaining}\")\n",
    "else:\n",
    "    log(\"\\n🎉 All files passed the HH threshold check!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2215e544-6803-4b72-9afb-fc5f500718cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#************************************************* energies and relaxation ****************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f327f11-3a91-4326-bba0-a52c4ec89335",
   "metadata": {},
   "outputs": [],
   "source": [
    "### function to count the total number of atoms\n",
    "\n",
    "\n",
    "def get_atom_count(xyz_file_path):\n",
    "    \"\"\"\n",
    "    Reads an XYZ file and returns the number of atoms.\n",
    "    \n",
    "    Parameters:\n",
    "        xyz_file_path (str): Path to the .xyz file.\n",
    "        \n",
    "    Returns:\n",
    "        int: Number of atoms in the file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(xyz_file_path, 'r') as file:\n",
    "            first_line = file.readline()\n",
    "            atom_count = int(first_line.strip())\n",
    "            return atom_count\n",
    "    except (FileNotFoundError, ValueError, IndexError) as e:\n",
    "        print(f\"Error reading {xyz_file_path}: {e}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2248581a-955b-44b4-842f-ce7e568281b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### get all the xyz files\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "i = 0\n",
    "num_atoms = []\n",
    "files_found = 0  # Counter for successfully found files\n",
    "\n",
    "for G in GRAPHS:\n",
    "    filename = f\"speciation{i}.xyz\"\n",
    "\n",
    "    if os.path.exists(filename):\n",
    "        num_atoms.append(get_atom_count(filename))\n",
    "        files_found += 1  # Increment counter\n",
    "    else:\n",
    "        num_atoms.append(0)\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "#print(\"Atom counts:\", num_atoms)\n",
    "print(f\"Total number of files found: {files_found}\")\n",
    "#print(f\"Total number of graphs processed: {len(GRAPHS)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25edc90-3496-46f2-804d-9d8a1fa59d52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### remove the dummy atoms if the !!!!continue here to comment\n",
    "\n",
    "\n",
    "if dummy_atom_needed != []:\n",
    "    for i in range(len(GRAPHS)):\n",
    "        filename = f\"speciation{i}.xyz\"\n",
    "\n",
    "        if not os.path.exists(filename):\n",
    "            print(f\"[WARNING] File not found: {filename}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            with open(filename, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "\n",
    "            # Replace occurrences in each line\n",
    "            updated_lines = [\n",
    "                line.replace(dummy_atom_needed[1], dummy_atom_needed[0])\n",
    "                for line in lines\n",
    "            ]\n",
    "\n",
    "            with open(filename, 'w') as f:\n",
    "                f.writelines(updated_lines)\n",
    "\n",
    "            print(f\"[INFO] Updated file: {filename}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to process {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed16df66-632a-441a-a0d4-261febd79213",
   "metadata": {
    "is_executing": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### uff ########\n",
    "\n",
    "# Function to compute UFF energy\n",
    "def compute_uff_energy(index, kj_to_kcal=1.0/4.184):\n",
    "    filename = f\"speciation{index}.xyz\"\n",
    "\n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"[WARNING] File not found: {filename}. Setting UFF energy to 999999999.\")\n",
    "        return 999999999\n",
    "\n",
    "    try:\n",
    "        mol = next(pybel.readfile(\"xyz\", filename))\n",
    "        obmol = mol.OBMol\n",
    "\n",
    "        ff = ob.OBForceField.FindForceField(\"uff\")\n",
    "        if not ff.Setup(obmol):\n",
    "            print(f\"[ERROR] UFF setup failed for molecule {index}. Setting UFF energy to 999999999.\")\n",
    "            return 999999999\n",
    "\n",
    "        ff.SetCoordinates(obmol)\n",
    "        uff_energy = ff.Energy(False) * kj_to_kcal  # False = no gradient calc\n",
    "        return uff_energy\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[EXCEPTION] Error processing {filename}: {e}. Setting UFF energy to 999999999.\")\n",
    "        return 999999999\n",
    "\n",
    "# Run UFF energy calculation in parallel\n",
    "num_molecules = len(GRAPHS)\n",
    "num_cores = joblib.cpu_count()\n",
    "\n",
    "E_gas_uff = Parallel(n_jobs=num_cores)(delayed(compute_uff_energy)(i) for i in range(num_molecules))\n",
    "\n",
    "# Output results\n",
    "for energy in E_gas_uff:\n",
    "    print(energy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635b66b5-a23f-4a58-9921-ba9989d7f9c1",
   "metadata": {
    "is_executing": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### saving the data, can rename the column name or file as wish\n",
    "\n",
    "df = pd.DataFrame({'atom_num': num_atoms, 'E_gas_uff': E_gas_uff})\n",
    "df.to_csv('E_gas-uff.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee3d33f-d021-4b5f-b0c3-03a8cb24e8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "### sorting all data including the index of the xyz file, atom number and uff energy\n",
    "\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "input_file = \"E_gas-uff.csv\"\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "\n",
    "atom_col = \"atom_num\"\n",
    "energy_col = \"E_gas_uff\"\n",
    "\n",
    "# Step 3: Organize data by atom number\n",
    "atom_data = defaultdict(lambda: {\"energy\": [], \"index\": []})\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    atom = int(row[atom_col])  # ensure integer atom_num\n",
    "    energy = row[energy_col]\n",
    "    atom_data[atom][\"energy\"].append(energy)\n",
    "    atom_data[atom][\"index\"].append(idx)\n",
    "\n",
    "\n",
    "pivot_data = {}\n",
    "max_len = 0\n",
    "\n",
    "for atom, data in atom_data.items():\n",
    "    sorted_pairs = sorted(zip(data[\"energy\"], data[\"index\"]))\n",
    "    sorted_energies, sorted_indices = zip(*sorted_pairs) if sorted_pairs else ([], [])\n",
    "    \n",
    "    pivot_data[f\"{atom}_energy\"] = list(sorted_energies)\n",
    "    pivot_data[f\"{atom}_index\"] = list(map(int, sorted_indices))  # convert to int\n",
    "    max_len = max(max_len, len(sorted_energies))\n",
    "\n",
    "for key in pivot_data:\n",
    "    padding = [None] * (max_len - len(pivot_data[key]))\n",
    "    pivot_data[key].extend(padding)\n",
    "\n",
    "output_df = pd.DataFrame(pivot_data)\n",
    "output_file = \"atom_energy_index_combined_output.csv\"\n",
    "output_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Final output saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e013787-6706-429b-8123-dd1f76c5cee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### reading the final sorted .csv and take the lowest energy structures evenly from each of the groups\n",
    "\n",
    "totla_structure_num = 100 ### here define the total number of structures taken to be modelled, this should be > no. of group * 3 at least \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the combined CSV\n",
    "input_file = \"atom_energy_index_combined_output.csv\"\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Initialize the list to hold index values\n",
    "collected_indices = []\n",
    "\n",
    "# Identify all columns that end with '_index'\n",
    "index_columns = [col for col in df.columns if col.endswith(\"_index\") and col != \"0_index\"]\n",
    "\n",
    "# Iterate row by row\n",
    "for _, row in df.iterrows():\n",
    "    for col in index_columns:\n",
    "        val = row[col]\n",
    "        if pd.notna(val):\n",
    "            collected_indices.append(int(val))\n",
    "        if len(collected_indices) >= totla_structure_num:\n",
    "            break\n",
    "    if len(collected_indices) >= totla_structure_num:\n",
    "        break\n",
    "\n",
    "# Display the collected list of indices (first few entries shown as preview)\n",
    "collected_indices[:10], len(collected_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6936768-ef09-47ad-b37f-b5aff89c63ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "######## costum creation of gaussian input files ##########\n",
    "### the user should have experience in creating gaussian input files using the vi. \n",
    "### please modify the level of theory, and core mem setting occurdingly.\n",
    "\n",
    "\n",
    "i=0\n",
    "name = 'speciation'\n",
    "\n",
    "\n",
    "for index in collected_indices:\n",
    "    new_comfile = name + str(index) + '.com'\n",
    "    open(new_comfile, '+a').write('%chk=' + name + str(index) + '.chk' + '\\n' + '%nprocshared=16\\n' + '%mem=32GB\\n' + '#p opt=loose PM7 freq scf=xqc \\n' + '\\n speciation study\\n' + '\\n-1 1\\n')\n",
    "    print(str(index))\n",
    "    \n",
    "    with open(\"speciation\" + str(index) + \".xyz\", 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        if lines.index(line) >= 2:\n",
    "            open(new_comfile, '+a').write(line)\n",
    "            \n",
    "    open(new_comfile, '+a').write('\\n')\n",
    "    i = i + 1\n",
    "\n",
    "\"\"\"atom_H = False\n",
    "atom_Li = False\n",
    "atom_K = False\n",
    "atom_C = False\n",
    "atom_N = False\n",
    "atom_O = False\n",
    "atom_F = False\n",
    "atom_Si = False\n",
    "atom_Co = False\n",
    "atom_U = False\n",
    "atom_Na = False\n",
    "atom_Mg = False\n",
    "\n",
    "#basis_Co = '-Co 0\\nlanl2dz\\nF 1 1.0\\n2.780 1.0\\n****\\n'\n",
    "basis_U = '-U 0\\nlanl2dz\\n****\\n'\n",
    "basis_K = '-K 0\\nlanl2dz\\n****\\n'\n",
    "basis_Li = '-Li 0\\n6-31g(d,p)\\n****\\n'\n",
    "#basis_K = '-K 0\\n6-31g(d,p)\\n****\\n'\n",
    "basis_Na = '-Na 0\\n6-31g(d,p)\\n****\\n'\n",
    "basis_Mg = '-Mg 0\\n6-31g(d,p)\\n****\\n'\n",
    "basis_H = '-H 0\\n6-31g(d,p)\\n****\\n'\n",
    "basis_C = '-C 0\\n6-31g(d,p)\\n****\\n'\n",
    "basis_N = '-N 0\\n6-31+g(d,p)\\n****\\n'\n",
    "basis_O = '-O 0\\n6-31+g(d,p)\\n****\\n'\n",
    "basis_F = '-F 0\\n6-31+g(d,p)\\n****\\n'\n",
    "basis_Si = '-Si 0\\n6-31g(d,p)\\n****\\n'\n",
    "\n",
    "\n",
    "i=0\n",
    "\n",
    "for index in collected_indices:\n",
    "    new_comfile = name + str(index) + '.com'\n",
    "    with open(new_comfile, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            if 'Mg    ' in line:\n",
    "                atom_Mg = True    \n",
    "            if atom_U:\n",
    "                open(new_comfile, '+a').write(basis_Mg)\n",
    "                break\n",
    "\n",
    "        for line in lines:\n",
    "            if 'H    ' in line:\n",
    "                atom_H = True\n",
    "            if atom_H:\n",
    "                open(new_comfile, '+a').write(basis_H)\n",
    "                break\n",
    "                \n",
    "        for line in lines:\n",
    "            if 'K    ' in line:\n",
    "                atom_K = True\n",
    "            if atom_K:\n",
    "                open(new_comfile, '+a').write(basis_K)\n",
    "                break\n",
    "                \n",
    "        for line in lines:\n",
    "            if 'Li    ' in line:\n",
    "                atom_Li = True\n",
    "            if atom_Li:\n",
    "                open(new_comfile, '+a').write(basis_Li)\n",
    "                break\n",
    "                \n",
    "        for line in lines:\n",
    "            if 'Na    ' in line:\n",
    "                atom_Na = True\n",
    "            if atom_Na:\n",
    "                open(new_comfile, '+a').write(basis_Na)\n",
    "                break\n",
    "    \n",
    "        for line in lines:\n",
    "            if 'C    ' in line:\n",
    "                atom_C = True\n",
    "            if atom_C:\n",
    "                open(new_comfile, '+a').write(basis_C)\n",
    "                break\n",
    "\n",
    "        for line in lines:\n",
    "            if 'N    ' in line:\n",
    "                atom_N = True\n",
    "            if atom_N:\n",
    "                open(new_comfile, '+a').write(basis_N)\n",
    "                break\n",
    "    \n",
    "        for line in lines:\n",
    "            if 'O    ' in line:\n",
    "                atom_O = True\n",
    "            if atom_O:\n",
    "                open(new_comfile, '+a').write(basis_O)\n",
    "                break\n",
    "\n",
    "        for line in lines:\n",
    "            if 'F    ' in line:\n",
    "                atom_F = True\n",
    "            if atom_F:\n",
    "                open(new_comfile, '+a').write(basis_F)\n",
    "                break            \n",
    "            \n",
    "        for line in lines:\n",
    "            if 'Si    ' in line:\n",
    "                atom_Si = True\n",
    "            if atom_Si:\n",
    "                open(new_comfile, '+a').write(basis_Si)\n",
    "                break            \n",
    "            \n",
    "            \n",
    "        i=i+1    \n",
    "        f.close()\"\"\"\n",
    "open(new_comfile, '+a').write('\\n\\n')\n",
    "    #open(new_comfile, '+a').write('\\nU\\nlanl2dz')\n",
    "    #open(new_comfile, '+a').write('\\nK\\nlanl2dz\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38eae91c-8960-4f4c-8ffa-e75e6f39c6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#************************************************* post Gaussian calculations ****************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adf7065-7724-4b83-8565-850cabcd8f15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### automating getting the Gibbs energy from the .log files \n",
    "\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def extract_scf_energy_and_natoms(filepath):\n",
    "    energy_pattern = re.compile(r'SCF Done:\\s+E\\(RPM7\\)\\s+=\\s+([-+]?\\d*\\.\\d+(?:[Ee][+-]?\\d+)?)') ### here can be changed to get the E(Done) if desired\n",
    "    natoms_pattern = re.compile(r'NAtoms=\\s+(\\d+)')\n",
    "    \n",
    "    last_energy = None\n",
    "    last_natoms = None\n",
    "\n",
    "    with open(filepath, 'r') as file:\n",
    "        for line in file:\n",
    "            energy_match = energy_pattern.search(line)\n",
    "            if energy_match:\n",
    "                last_energy = float(energy_match.group(1))\n",
    "            natoms_match = natoms_pattern.search(line)\n",
    "            if natoms_match:\n",
    "                last_natoms = int(natoms_match.group(1))\n",
    "\n",
    "    return last_energy, last_natoms\n",
    "\n",
    "\n",
    "directory_path = \"/Users/mantingmu/Desktop/CASE3_Mg2IIP/SSIP/anion2/PM7-log\"  ### provide the actual path\n",
    "grouped_results = defaultdict(list)\n",
    "\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith(\".log\"):\n",
    "        cleaned_name = filename.replace(\"speciation\", \"\")\n",
    "        if cleaned_name.endswith(\"b.log\"):\n",
    "            cleaned_name = cleaned_name.replace(\"b.log\", \"\").replace(\"b\", \"\")\n",
    "        elif cleaned_name.endswith(\".log\"):\n",
    "            cleaned_name = cleaned_name.replace(\".log\", \"\")\n",
    "        \n",
    "        full_path = os.path.join(directory_path, filename)\n",
    "        #print (full_path)\n",
    "        energy, natoms = extract_scf_energy_and_natoms(full_path)\n",
    "\n",
    "        if energy is not None and natoms is not None:\n",
    "            grouped_results[natoms].append((cleaned_name, energy, natoms))\n",
    "\n",
    "sorted_index = []\n",
    "\n",
    "### Sort and display the lowest energy entries per natoms group (3 by default), which will subject to further modelling\n",
    "structures_per_group = 3  ### change here if would like to sample more\n",
    "\n",
    "\n",
    "for natoms in sorted(grouped_results.keys()):\n",
    "    group = grouped_results[natoms]\n",
    "    #print (group)\n",
    "    sorted_group = sorted(group, key=lambda x: x[1])  # sort by energy\n",
    "\n",
    "    print(f\"\\nTop 3 lowest SCF energies for NAtoms = {natoms}:\")\n",
    "    for entry in sorted_group[:structures_per_group]:  \n",
    "        name, energy, _ = entry\n",
    "        sorted_index.append(name)\n",
    "        print(f\"  {name.strip()}: {energy:.12f}\")\n",
    "\n",
    "output_dict = {}\n",
    "\n",
    "for natoms in sorted(grouped_results.keys()):\n",
    "    sorted_group = sorted(grouped_results[natoms], key=lambda x: x[1])\n",
    "    indices = [str(entry[0]) for entry in sorted_group[:structures_per_group]]\n",
    "    energies = [f\"{entry[1]:.12f}\" for entry in sorted_group[:structures_per_group]]\n",
    "\n",
    "    # pad to 3 entries\n",
    "    while len(indices) < structures_per_group:\n",
    "        indices.append(\"\")\n",
    "        energies.append(\"\")\n",
    "\n",
    "    output_dict[f\"{natoms}_index\"] = indices\n",
    "    output_dict[f\"{natoms}_energy\"] = energies\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(output_dict)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"top3_grouped_by_natoms.csv\", index=False)\n",
    "\n",
    "print(\"CSV saved as 'top3_grouped_by_natoms.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109f60ef-b690-4c2f-be5e-a4a0bfce00c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted_index) ### just to double check again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132b8260-0484-4d52-8e62-0e6e0e9d22a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### taking atructures from .log files and write new .com files with higher level of theory\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "# Predefined values\n",
    "name = 'speciationDFT'\n",
    "\n",
    "# atomic number to symbol mapping please add more as go\n",
    "atomic_number_to_symbol = {\n",
    "    1: 'H', 3: 'Li', 6: 'C', 7: 'N', 8: 'O', 9: 'F',\n",
    "    11: 'Na', 12: 'Mg', 14: 'Si', 19: 'K', 27: 'Co'\n",
    "}\n",
    "\n",
    "# Basis set definitions\n",
    "atom_basis = {\n",
    "    'H': '-H 0\\n6-31g(d,p)\\n****\\n',\n",
    "    'Li': '-Li 0\\n6-31g(d,p)\\n****\\n',\n",
    "    'K': '-K 0\\n6-31g(d,p)\\n****\\n',\n",
    "    'Na': '-Na 0\\n6-31g(d,p)\\n****\\n',\n",
    "    'C': '-C 0\\n6-31g(d,p)\\n****\\n',\n",
    "    'N': '-N 0\\n6-31+g(d,p)\\n****\\n',\n",
    "    'O': '-O 0\\n6-31+g(d,p)\\n****\\n',\n",
    "    'F': '-F 0\\n6-31+g(d,p)\\n****\\n',\n",
    "    'Si': '-Si 0\\n6-31g(d,p)\\n****\\n',\n",
    "    'Mg': '-Mg 0\\n6-31g(d,p)\\n****\\n',\n",
    "    'Co': '-Co 0\\nlanl2dz\\nF 1 1.0\\n2.780 1.0\\n****\\n'\n",
    "}\n",
    "\n",
    "def extract_last_geometry_from_log(full_path):\n",
    "    with open(full_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    start_indices = [i for i, line in enumerate(lines) if 'Standard orientation:' in line]\n",
    "    if not start_indices:\n",
    "        return []\n",
    "\n",
    "    geom_start = start_indices[-1] + 5\n",
    "    geom = []\n",
    "\n",
    "    for line in lines[geom_start:]:\n",
    "        if '-----' in line or line.strip() == '':\n",
    "            break\n",
    "        parts = line.split()\n",
    "        atom_num = int(parts[1])\n",
    "        x, y, z = map(float, parts[3:6])\n",
    "        element = atomic_number_to_symbol.get(atom_num, 'X')  # fallback to 'X'\n",
    "        geom.append(f\"{element} {x:.6f} {y:.6f} {z:.6f}\")\n",
    "\n",
    "    return geom\n",
    "\n",
    "def find_log_file_with_fallback(directory_path, base_filename):\n",
    "    \"\"\"\n",
    "    Try finding the .log file by appending up to four 'b's before the extension if needed.\n",
    "    \"\"\"\n",
    "    for i in range(5):  # 0 to 4 b's\n",
    "        b_suffix = 'b' * i\n",
    "        filename = f\"{base_filename}{b_suffix}.log\"\n",
    "        full_path = os.path.join(directory_path, filename)\n",
    "        if os.path.exists(full_path):\n",
    "            return full_path, filename\n",
    "    return None, None\n",
    "\n",
    "# Main loop\n",
    "for index in sorted_index:\n",
    "    base_filename = f\"speciation{index}\"\n",
    "    full_path, filename = find_log_file_with_fallback(directory_path, base_filename)\n",
    "    com_file = f\"{name}{index}.com\"\n",
    "\n",
    "    if not full_path:\n",
    "        print(f\"❌ File not found: tried {base_filename}[b].log up to 4 bs\")\n",
    "        continue\n",
    "\n",
    "    geometry_lines = extract_last_geometry_from_log(full_path)\n",
    "    if not geometry_lines:\n",
    "        print(f\"⚠️  No geometry found in {filename}\")\n",
    "        continue\n",
    "\n",
    "    with open(com_file, 'w') as f:\n",
    "        f.write(f\"%chk={name}{index}.chk\\n\")\n",
    "        f.write(\"%nprocshared=32\\n\")\n",
    "        f.write(\"%mem=64GB\\n\")\n",
    "        f.write(\"#p opt=loose wb97xd/gen scf=xqc freq\\n\\n\")\n",
    "        f.write(\"speciation study\\n\\n\")\n",
    "        f.write(\"0 1\\n\")\n",
    "        for line in geometry_lines:\n",
    "            f.write(line + \"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "        atoms_present = set(line.split()[0] for line in geometry_lines)\n",
    "        for atom in atoms_present:\n",
    "            if atom in atom_basis:\n",
    "                f.write(atom_basis[atom])\n",
    "\n",
    "        f.write(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e52e561-4bce-4621-af3c-7045aebef51d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### again creating the .csv with the results from calculations\n",
    "\n",
    "\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def extract_scf_energy_and_natoms(filepath):\n",
    "    energy_pattern = re.compile(r'SCF Done:\\s+E\\(\\w+\\)\\s+=\\s+([-+]?\\d+\\.\\d+)\\s+A\\.U\\.')\n",
    "    natoms_pattern = re.compile(r'NAtoms=\\s+(\\d+)')\n",
    "    \n",
    "    last_energy = None\n",
    "    last_natoms = None\n",
    "\n",
    "    with open(filepath, 'r') as file:\n",
    "        for line in file:\n",
    "            energy_match = energy_pattern.search(line)\n",
    "            if energy_match:\n",
    "                last_energy = float(energy_match.group(1))\n",
    "            natoms_match = natoms_pattern.search(line)\n",
    "            if natoms_match:\n",
    "                last_natoms = int(natoms_match.group(1))\n",
    "\n",
    "    return last_energy, last_natoms\n",
    "\n",
    "\n",
    "directory_path = \"/Users/mantingmu/Desktop/CASE3_Mg2IIP/SSIP/anion2/dft/\" ### modify\n",
    "grouped_results = defaultdict(list)\n",
    "\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith(\".log\"):\n",
    "        cleaned_name = filename.replace(\"speciationDFT\", \"\")\n",
    "        if cleaned_name.endswith(\"b.log\"):\n",
    "            cleaned_name = cleaned_name.replace(\"b.log\", \"\").replace(\"b\", \"\")\n",
    "        elif cleaned_name.endswith(\".log\"):\n",
    "            cleaned_name = cleaned_name.replace(\".log\", \"\")\n",
    "        \n",
    "        full_path = os.path.join(directory_path, filename)\n",
    "        #print (full_path)\n",
    "        energy, natoms = extract_scf_energy_and_natoms(full_path)\n",
    "\n",
    "        if energy is not None and natoms is not None:\n",
    "            grouped_results[natoms].append((cleaned_name, energy, natoms))\n",
    "\n",
    "sorted_index = []\n",
    "\n",
    "# Sort and display the top 3 lowest energy entries per natoms group\n",
    "for natoms in sorted(grouped_results.keys()):\n",
    "    group = grouped_results[natoms]\n",
    "    #print (group)\n",
    "    sorted_group = sorted(group, key=lambda x: x[1])  # sort by energy\n",
    "\n",
    "    print(f\"\\nTop 3 lowest SCF energies for NAtoms = {natoms}:\")\n",
    "    for entry in sorted_group[:3]:\n",
    "        name, energy, _ = entry\n",
    "        sorted_index.append(name)\n",
    "        print(f\"  {name.strip()}: {energy:.12f}\")\n",
    "\n",
    "output_dict = {}\n",
    "\n",
    "for natoms in sorted(grouped_results.keys()):\n",
    "    sorted_group = sorted(grouped_results[natoms], key=lambda x: x[1])\n",
    "    indices = [str(entry[0]) for entry in sorted_group[:3]]\n",
    "    energies = [f\"{entry[1]:.12f}\" for entry in sorted_group[:3]]\n",
    "\n",
    "    # pad to 3 entries\n",
    "    while len(indices) < 3:\n",
    "        indices.append(\"\")\n",
    "        energies.append(\"\")\n",
    "\n",
    "    output_dict[f\"{natoms}_index\"] = indices\n",
    "    output_dict[f\"{natoms}_energy\"] = energies\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(output_dict)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"/Users/mantingmu/Desktop/CASE3_Mg2IIP/SSIP/anion2/DFT_Energies.csv\", index=False) ### modify\n",
    "\n",
    "print(\"CSV saved as 'DFT_Energies.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755ce617-5d2e-480f-84e8-605168ea5a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#************************************************* IIP structures ****************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4ebb2a-76a2-45ef-aae9-42758972e792",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### here we will generat all the IIP structures\n",
    "### note the SSIPs should be at least generated by SPECI, optimised with appropiate level of theory is recommanded prior to this\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from ase import Atoms\n",
    "from ase.io import read, write\n",
    "from itertools import product\n",
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "import tempfile\n",
    "from openbabel import pybel, openbabel as ob\n",
    "import contextlib\n",
    "import io\n",
    "\n",
    "kj_to_kcal = 0.239005736\n",
    "\n",
    "######### Utility Functions #########\n",
    "\n",
    "def find_closest_atoms(cation, anion):\n",
    "    min_distance = np.inf\n",
    "    idx_c, idx_a = -1, -1\n",
    "    for i, atom_c in enumerate(cation):\n",
    "        for j, atom_a in enumerate(anion):\n",
    "            distance = np.linalg.norm(atom_a.position - atom_c.position)\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                idx_c, idx_a = i, j\n",
    "    return idx_c, idx_a\n",
    "\n",
    "def adjust_positions(cation, anion, initial_distance, final_distance):\n",
    "    com_cation = cation.get_center_of_mass()\n",
    "    com_anion = anion.get_center_of_mass()\n",
    "    initial_vector = com_anion - com_cation\n",
    "    norm_initial_vector = initial_vector / np.linalg.norm(initial_vector)\n",
    "    anion.translate(norm_initial_vector * (initial_distance - np.linalg.norm(initial_vector)))\n",
    "\n",
    "    idx_c, idx_a = find_closest_atoms(cation, anion)\n",
    "    atom_c = cation[idx_c].position\n",
    "    atom_a = anion[idx_a].position\n",
    "    vector = atom_a - atom_c\n",
    "    norm_vector = vector / np.linalg.norm(vector)\n",
    "    displacement = (final_distance - np.linalg.norm(atom_a - atom_c)) * norm_vector\n",
    "    anion.translate(displacement)\n",
    "\n",
    "######### UFF Optimized Placement Function #########\n",
    "\n",
    "def random_placement_all(cation, anion, initial_distance=50.0, final_distance=2.5): ### edit final distance here \n",
    "    structures_with_energies = []\n",
    "    rotation_angles = [0, 120, 240]  ### edit rotation angles as wish: default is 3x3x3 points\n",
    "\n",
    "    for cx in rotation_angles:\n",
    "        for cy in rotation_angles:\n",
    "            for cz in rotation_angles:\n",
    "                temp_cation = cation.copy()\n",
    "                temp_cation.rotate(cx, 'x', center='COM')\n",
    "                temp_cation.rotate(cy, 'y', center='COM')\n",
    "                temp_cation.rotate(cz, 'z', center='COM')\n",
    "\n",
    "                for ax in rotation_angles:\n",
    "                    for ay in rotation_angles:\n",
    "                        for az in rotation_angles:\n",
    "                            temp_anion = anion.copy()\n",
    "                            temp_anion.rotate(ax, 'x', center='COM')\n",
    "                            temp_anion.rotate(ay, 'y', center='COM')\n",
    "                            temp_anion.rotate(az, 'z', center='COM')\n",
    "\n",
    "                            adjusted_anion = temp_anion.copy()\n",
    "                            adjust_positions(temp_cation, adjusted_anion, initial_distance, final_distance)\n",
    "\n",
    "                            combined = temp_cation + adjusted_anion\n",
    "\n",
    "                            try:\n",
    "                                with tempfile.NamedTemporaryFile(delete=False, suffix=\".xyz\", mode='w') as tmp:\n",
    "                                    tmp_name = tmp.name\n",
    "                                    write(tmp, combined, format='xyz')\n",
    "\n",
    "                                mol = next(pybel.readfile(\"xyz\", tmp_name))\n",
    "                                obmol = mol.OBMol\n",
    "                                ff = ob.OBForceField.FindForceField(\"uff\")\n",
    "\n",
    "                                with contextlib.redirect_stderr(io.StringIO()):\n",
    "                                    uff_ok = ff.Setup(obmol)\n",
    "\n",
    "                                if not uff_ok:\n",
    "                                    print(\"[ERROR] UFF setup failed. Skipping...\")\n",
    "                                    current_energy = 999999999\n",
    "                                else:\n",
    "                                    with contextlib.redirect_stderr(io.StringIO()):\n",
    "                                        ff.SteepestDescent(50)\n",
    "                                        ff.GetCoordinates(obmol)\n",
    "                                        current_energy = ff.Energy(False) * kj_to_kcal\n",
    "                                        mol.write(\"xyz\", tmp_name, overwrite=True)\n",
    "\n",
    "                                    try:\n",
    "                                        combined_optimized = read(tmp_name)\n",
    "                                        if len(combined_optimized) == 0:\n",
    "                                            print(\"[WARNING] Optimized structure is empty. Using pre-optimized.\")\n",
    "                                            combined_optimized = combined\n",
    "                                    except Exception as e:\n",
    "                                        print(f\"[WARNING] Reading optimized structure failed: {e}\")\n",
    "                                        combined_optimized = combined\n",
    "\n",
    "                                    combined = combined_optimized\n",
    "\n",
    "                                os.remove(tmp_name)\n",
    "\n",
    "                            except Exception as e:\n",
    "                                print(f\"[EXCEPTION] UFF optimization failed: {e}\")\n",
    "                                current_energy = 999999999\n",
    "\n",
    "                            structures_with_energies.append((combined.copy(), current_energy))\n",
    "\n",
    "    return structures_with_energies\n",
    "\n",
    "######### Redundant Filtering #########\n",
    "\n",
    "def unique_multisets(elements, r):\n",
    "    seen = set()\n",
    "    for combo in product(elements, repeat=r):\n",
    "        key = tuple(sorted(Counter(combo).items()))\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            yield combo\n",
    "\n",
    "######### Main Logic #########\n",
    "\n",
    "def generate_combinations(base_dir, cation_count, anion_count):\n",
    "    cat_dir = os.path.join(base_dir, 'cat')\n",
    "    ani_dir = os.path.join(base_dir, 'ani')\n",
    "    output_dir = os.path.join(base_dir, 'generated_xyz')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    cat_files = sorted([f for f in os.listdir(cat_dir) if f.endswith('.xyz')])\n",
    "    ani_files = sorted([f for f in os.listdir(ani_dir) if f.endswith('.xyz')])\n",
    "\n",
    "    if not cat_files or not ani_files:\n",
    "        raise ValueError(\"No .xyz files found in 'cat/' or 'ani/'\")\n",
    "\n",
    "    cat_combos = list(unique_multisets(cat_files, cation_count))\n",
    "    ani_combos = list(unique_multisets(ani_files, anion_count))\n",
    "\n",
    "    output_index = 0\n",
    "\n",
    "    for cat_group in cat_combos:\n",
    "        print (cat_group)\n",
    "        for ani_group in ani_combos:\n",
    "            print (ani_group)\n",
    "            cation0 = read(os.path.join(cat_dir, cat_group[0]))\n",
    "            anion0 = read(os.path.join(ani_dir, ani_group[0]))\n",
    "            initial_structures = random_placement_all(cation0, anion0)\n",
    "\n",
    "            if cation_count > 1 or anion_count > 1:\n",
    "                initial_structures = [min(initial_structures, key=lambda x: x[1])]\n",
    "\n",
    "            all_final_structs = []\n",
    "\n",
    "            for base_struct, base_energy in initial_structures:\n",
    "                current_structs = [(base_struct, base_energy)]\n",
    "\n",
    "                for i in range(1, anion_count):\n",
    "                    next_anion = read(os.path.join(ani_dir, ani_group[i]))\n",
    "                    new_structs = []\n",
    "                    for struct, _ in current_structs:\n",
    "                        added = random_placement_all(next_anion, struct)\n",
    "                        new_structs.extend(added)\n",
    "\n",
    "                    # Keep only most stable if more than 2 anions\n",
    "                    if anion_count > 2:\n",
    "                        current_structs = [min(new_structs, key=lambda x: x[1])]\n",
    "                    else:\n",
    "                        current_structs = new_structs\n",
    "\n",
    "                for j in range(1, cation_count):\n",
    "                    next_cation = read(os.path.join(cat_dir, cat_group[j]))\n",
    "                    new_structs = []\n",
    "                    for struct, _ in current_structs:\n",
    "                        added = random_placement_all(next_cation, struct)\n",
    "                        new_structs.extend(added)\n",
    "\n",
    "                    # Keep only most stable if more than 2 cations\n",
    "                    if cation_count > 2:\n",
    "                        current_structs = [min(new_structs, key=lambda x: x[1])]\n",
    "                    else:\n",
    "                        current_structs = new_structs\n",
    "\n",
    "                all_final_structs.extend(current_structs)\n",
    "\n",
    "            sorted_finals = sorted(all_final_structs, key=lambda x: x[1])[:10] ### edit here to keep more than 10 if desired \n",
    "\n",
    "            trace = \"_\".join([f.replace(\".xyz\", \"\").replace(\" \", \"\") for f in (cat_group + ani_group)])\n",
    "            for rank, (structure, energy) in enumerate(sorted_finals):\n",
    "                filename = (\n",
    "                    f\"lowest10_c{cation_count}_a{anion_count}_\"\n",
    "                    f\"{trace}_rank{rank+1:02d}_E{energy:.2f}.xyz\"\n",
    "                )\n",
    "                full_path = os.path.join(output_dir, filename)\n",
    "                write(full_path, structure, format='xyz')\n",
    "            print ('output_index = ', output_index)\n",
    "            output_index += 1\n",
    "\n",
    "# === USAGE ===\n",
    "### one should have in the base_dir a folder named \"ani\" and a foler named \"cat\", the ouput IIP xyzs will be in the \"generated_xyz\" folder\n",
    "base_dir = '/Users/mantingmu/Desktop/CASE3_Mg2IIP/IIP/cat2ani2' ### please edit here \n",
    "generate_combinations(base_dir, cation_count=1, anion_count=1) ### modify the cation and anion _count here to specify the numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9055d547-5c66-46ad-bec6-201013a05496",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = os.path.join(base_dir, 'generated_xyz')\n",
    "print(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63db8dc-9044-43c2-8718-1af9c18b204c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### automated generation of gaussian input file \n",
    "\n",
    "\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "######### Configuration #########\n",
    "xyz_dir = 'generated_xyz'\n",
    "output_dir = 'gaussian_inputs'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "nproc = 20\n",
    "mem = '40GB'\n",
    "route_section = '#p opt=loose PM7/gen scf=xqc '\n",
    "\n",
    "######### Basis Set Definitions #########\n",
    "basis_sets = {\n",
    "    'H':  '-H 0\\n6-31g(d,p)\\n****\\n',\n",
    "    'Li': '-Li 0\\n6-31g(d,p)\\n****\\n',\n",
    "    'K':  '-K 0\\n6-31g(d,p)\\n****\\n',\n",
    "    'Na': '-Na 0\\n6-31g(d,p)\\n****\\n',\n",
    "    'C':  '-C 0\\n6-31g(d,p)\\n****\\n',\n",
    "    'N':  '-N 0\\n6-31+g(d,p)\\n****\\n',\n",
    "    'O':  '-O 0\\n6-31+g(d,p)\\n****\\n',\n",
    "    'F':  '-F 0\\n6-31+g(d,p)\\n****\\n',\n",
    "    'Si': '-Si 0\\n6-31g(d,p)\\n****\\n',\n",
    "    'Mg': '-Mg 0\\n6-31g(d,p)\\n****\\n',\n",
    "    'Co': '-Co 0\\nlanl2dz\\nF 1 1.0\\n2.780 1.0\\n****\\n'\n",
    "}\n",
    "\n",
    "######### Processing Each .xyz File #########\n",
    "xyz_files = [f for f in os.listdir(output_path) if f.endswith('.xyz')]\n",
    "\n",
    "if not xyz_files:\n",
    "    raise RuntimeError(f\"No .xyz files found in '{xyz_dir}'\")\n",
    "\n",
    "for xyz_file in sorted(xyz_files):\n",
    "    xyz_path = os.path.join(output_path, xyz_file)\n",
    "    base_name = os.path.splitext(xyz_file)[0]\n",
    "    com_filename = base_name + '.com'\n",
    "    com_path = os.path.join(output_path, com_filename)\n",
    "    chk_name = base_name + '.chk'\n",
    "\n",
    "    # Read XYZ geometry\n",
    "    with open(xyz_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        atom_lines = lines[2:]  # skip first two header lines\n",
    "\n",
    "    # Detect which atoms are present\n",
    "    present_atoms = set()\n",
    "    for line in atom_lines:\n",
    "        parts = line.strip().split()\n",
    "        if parts:\n",
    "            atom = parts[0]\n",
    "            if atom in basis_sets:\n",
    "                present_atoms.add(atom)\n",
    "\n",
    "    # Write Gaussian input (.com)\n",
    "    with open(com_path, 'w') as com:\n",
    "        # Header\n",
    "        com.write(f'%chk={chk_name}\\n')\n",
    "        com.write(f'%nprocshared={nproc}\\n')\n",
    "        com.write(f'%mem={mem}\\n')\n",
    "        com.write(f'{route_section}\\n\\n')\n",
    "        com.write(f'{base_name} DFT input\\n\\n')\n",
    "        com.write('0 1\\n')\n",
    "\n",
    "        # Geometry\n",
    "        for line in atom_lines:\n",
    "            com.write(line)\n",
    "\n",
    "        com.write('\\n')\n",
    "\n",
    "        # Basis Sets\n",
    "        for atom in sorted(present_atoms):\n",
    "            com.write(basis_sets[atom])\n",
    "\n",
    "        com.write('\\n\\n')\n",
    "\n",
    "    print(f\"✅ Generated: {com_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf9a53c-cbe6-4687-bb90-7d02b017f25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#************************************************* preparing for COPASI calculation applies to CIP, SSIP and IIPs ****************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e659bc-89eb-43be-8384-9edb88f2a06e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### getting all the Gibbs energies from the output files\n",
    "\n",
    "\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "def extract_scf_energy_and_natoms(filepath):\n",
    "    # Match the Free Energy line and capture the number (can be negative, with decimals)\n",
    "    energy_pattern = re.compile(\n",
    "        r\"Sum of electronic and thermal Free Energies=\\s*([-+]?\\d+\\.\\d+)\"\n",
    "    )\n",
    "    # Match NAtoms=\n",
    "    natoms_pattern = re.compile(r\"NAtoms=\\s+(\\d+)\")\n",
    "    \n",
    "    last_energy = None\n",
    "    last_natoms = None\n",
    "\n",
    "    with open(filepath, 'r') as file:\n",
    "        for line in file:\n",
    "            em = energy_pattern.search(line)\n",
    "            if em:\n",
    "                last_energy = float(em.group(1))\n",
    "            nm = natoms_pattern.search(line)\n",
    "            if nm:\n",
    "                last_natoms = int(nm.group(1))\n",
    "\n",
    "    return last_energy, last_natoms\n",
    "\n",
    "\n",
    "directory_path = \"/Users/mantingmu/Desktop/CASE3_Mg2IIP/CIP/PM7-Gibbs/\" ## add the location of log files\n",
    "grouped_results = defaultdict(list)\n",
    "\n",
    "for filename in os.listdir(directory_path):\n",
    "    if not filename.endswith(\".log\"):\n",
    "        continue\n",
    "\n",
    "    # clean up the name\n",
    "    name = filename.replace(\"speciationDFT\", \"\")\n",
    "    \n",
    "    #####specific for IIP########\n",
    "    #name = name.replace(\"lowest10_c2_a1_\", \"\")\n",
    "    name = name.replace(\"speciation\", \"\")\n",
    "    #name = name.replace(\"_\", \"\")\n",
    "    #name = name.replace(\"rank\", \"\")\n",
    "    #name = name.replace(\"E\", \"\")\n",
    "    #name = name.replace(\".\", \"\")\n",
    "    #name = name.replace(\"b\", \"\")\n",
    "    #####specific for IIP########\n",
    "    \n",
    "    if name.endswith(\"b.log\"):\n",
    "        name = name[:-5].rstrip(\"b\")\n",
    "    else:\n",
    "        name = name[:-4]\n",
    "\n",
    "    full_path = os.path.join(directory_path, filename)\n",
    "    energy, natoms = extract_scf_energy_and_natoms(full_path)\n",
    "\n",
    "    if energy is not None and natoms is not None:\n",
    "        grouped_results[natoms].append((name, energy, natoms))\n",
    "\n",
    "sorted_index = []\n",
    "output_dict = {}\n",
    "\n",
    "# For each natoms, pick the 3 lowest‐energy structures\n",
    "for natoms in sorted(grouped_results):\n",
    "    group = sorted(grouped_results[natoms], key=lambda x: x[1])\n",
    "    top3 = group[:100]\n",
    "\n",
    "    # Print to console\n",
    "    print(f\"\\nTop 3 lowest SCF energies for NAtoms = {natoms}:\")\n",
    "    for name, energy, _ in top3:\n",
    "        sorted_index.append(name)\n",
    "        print(f\"  {name}: {energy:.12f}\")\n",
    "\n",
    "    # Prepare for DataFrame\n",
    "    idxs = [entry[0] for entry in top3]\n",
    "    eners = [f\"{entry[1]:.12f}\" for entry in top3]\n",
    "    # pad if fewer than 3\n",
    "    while len(idxs) < 100:\n",
    "        idxs.append(\"\")\n",
    "        eners.append(\"\")\n",
    "\n",
    "    output_dict[f\"{natoms}_index\"]  = idxs\n",
    "    output_dict[f\"{natoms}_energy\"] = eners\n",
    "\n",
    "# Build DataFrame and save\n",
    "df_out = pd.DataFrame(output_dict)\n",
    "#out_csv = \"/Users/mantingmu/Desktop/CASE3_Mg2IIP/IIP/cat1cat1ani2/PM7/DFTGibbs_Energies.csv\"\n",
    "out_csv = \"/Users/mantingmu/Desktop/Energies.csv\" ###give location of output\n",
    "df_out.to_csv(out_csv, index=False)\n",
    "print(f\"\\nCSV saved as '{out_csv}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516f2fad-47fe-483a-9464-32d069d6c96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### getting all atomic compositions output files function\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def count_elements_from_log(log_path):\n",
    "    with open(log_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    start_index = None\n",
    "    for i, line in enumerate(lines):\n",
    "        if \"Symbolic Z-matrix:\" in line:\n",
    "            start_index = i + 1\n",
    "            break\n",
    "\n",
    "    if start_index is None:\n",
    "        raise ValueError(\"Start of XYZ section not found.\")\n",
    "\n",
    "    element_counts = Counter()\n",
    "    for line in lines[start_index:]:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            break  # stop at first empty line\n",
    "        parts = line.split()\n",
    "        if parts:\n",
    "            element = parts[0]\n",
    "            element_counts[element] += 1\n",
    "\n",
    "    return dict(element_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05690418-b8e0-4774-abd8-2a77eed6b543",
   "metadata": {},
   "outputs": [],
   "source": [
    "### actually getting all atomic compositions output files\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "def count_elements_from_log(log_path):\n",
    "    with open(log_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    start_index = None\n",
    "    # First find \"Charge = ...\" line, then start reading from the next non-empty line\n",
    "    for i, line in enumerate(lines):\n",
    "        if \"Charge =\" in line:\n",
    "            # skip any subsequent empty lines to find first atom line\n",
    "            j = i + 1\n",
    "            while j < len(lines) and not lines[j].strip():\n",
    "                j += 1\n",
    "            start_index = j\n",
    "            break\n",
    "\n",
    "    if start_index is None:\n",
    "        raise ValueError(\"Charge line not found — cannot determine start of XYZ section.\")\n",
    "\n",
    "    element_counts = Counter()\n",
    "    for line in lines[start_index:]:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            break  # stop at first empty line\n",
    "        parts = line.split()\n",
    "        if parts:\n",
    "            element = parts[0]\n",
    "            element_counts[element] += 1\n",
    "\n",
    "    return dict(element_counts)\n",
    "\n",
    "# Define the path to the directory\n",
    "#directory_path = \"/Users/mantingmu/Desktop/CASE3_Mg2IIP/CIP/PM7-Gibbs/\"\n",
    "\n",
    "# Store results with filename and element counts\n",
    "results = []\n",
    "\n",
    "# Ensure the directory exists before continuing\n",
    "if os.path.exists(directory_path):\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".log\"):\n",
    "            cleaned_name = filename.replace(\"speciationDFT\", \"\")\n",
    "\n",
    "            \n",
    "            #####specific for IIP########\n",
    "            #cleaned_name = cleaned_name.replace(\"lowest10_c2_a1_\", \"\")\n",
    "            cleaned_name = cleaned_name.replace(\"speciation\", \"\")\n",
    "            #cleaned_name = cleaned_name.replace(\"_\", \"\")\n",
    "            #cleaned_name = cleaned_name.replace(\"rank\", \"\")\n",
    "            #cleaned_name = cleaned_name.replace(\"E\", \"\")\n",
    "            #cleaned_name = cleaned_name.replace(\".\", \"\")\n",
    "            #cleaned_name = cleaned_name.replace(\"b\", \"\")\n",
    "            #####specific for IIP########\n",
    "\n",
    "            \n",
    "            if cleaned_name.endswith(\"b.log\"):\n",
    "                cleaned_name = cleaned_name[:-5].rstrip(\"b\")\n",
    "            else:\n",
    "                cleaned_name = cleaned_name[:-4]\n",
    "            \n",
    "            full_path = os.path.join(directory_path, filename)\n",
    "            \n",
    "            try:\n",
    "                elements = count_elements_from_log(full_path)\n",
    "                row = {\"Index\": cleaned_name}\n",
    "                row.update(elements)\n",
    "                results.append(row)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Save to CSV\n",
    "csv_path = \"/Users/mantingmu/Desktop/elemental_composition.csv\" ###give location of output \n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(\"Saved to:\", csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a96b49a-3f0b-4639-b56a-68e6d09542a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### collecting and store, energy, index, composition in a same csv file\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "energy_csv = \"/Users/mantingmu/Desktop/Energies.csv\" ###give the location of this .csv file\n",
    "df_energy = pd.read_csv(energy_csv)\n",
    "\n",
    "\n",
    "first_row = df_energy.iloc[0]\n",
    "index_energy_pairs = []\n",
    "\n",
    "for i in range(0, len(first_row), 2):\n",
    "    index_val = int(first_row[i])\n",
    "    energy_val = first_row[i + 1]\n",
    "    index_energy_pairs.append((index_val, energy_val))\n",
    "\n",
    "\n",
    "df_energy_filtered = pd.DataFrame(index_energy_pairs, columns=[\"Index\", \"Energy\"])\n",
    "composition_csv = \"/Users/mantingmu/Desktop/elemental_composition.csv\" ###and provide location here\n",
    "df_comp = pd.read_csv(composition_csv)\n",
    "df_filtered = df_comp[df_comp[\"Index\"].astype(int).isin(df_energy_filtered[\"Index\"])]\n",
    "df_filtered = df_filtered.merge(df_energy_filtered, on=\"Index\", how=\"left\")\n",
    "cols = [\"Index\", \"Energy\"] + [col for col in df_filtered.columns if col not in [\"Index\", \"Energy\"]]\n",
    "df_filtered = df_filtered[cols]\n",
    "df_filtered = df_filtered.loc[:, ~df_filtered.columns.str.contains('^Unnamed')]\n",
    "output_csv = \"/Users/mantingmu/Desktop/filtered_elemental_composition.csv\"\n",
    "df_filtered.to_csv(output_csv, index=False)\n",
    "\n",
    "print(\"Saved:\", output_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b71dbe-7883-423c-89d1-4d4e4ecfa157",
   "metadata": {},
   "outputs": [],
   "source": [
    "#************************************************* build elementary reaction steps ****************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8b5399-71cf-4d71-ae37-63a4e5e7dd85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### important here to add to the filtered_elemental_composition.csv any energies computed for neutral donor liagnds that could be dissociated\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "csv_path = output_csv\n",
    "df = pd.read_csv(csv_path, index_col=0)\n",
    "\n",
    "energies = df['Energy'].to_dict()\n",
    "element_cols = [col for col in df.columns if col != 'Energy']\n",
    "df[element_cols] = df[element_cols].fillna(0)\n",
    "compositions = {idx: tuple(df.loc[idx, element_cols]) for idx in df.index}\n",
    "\n",
    "def sum_comp(species_ids):\n",
    "    return tuple(\n",
    "        sum(compositions[i][j] for i in species_ids)\n",
    "        for j in range(len(element_cols))\n",
    "    )\n",
    "\n",
    "species = list(compositions.keys())\n",
    "reactant_combos = list(itertools.combinations_with_replacement(species, 2)) + [(i,) for i in species]\n",
    "product_combos = reactant_combos.copy()\n",
    "rows = []\n",
    "for r in reactant_combos:\n",
    "    for p in product_combos:\n",
    "        # Check elemental balance and remove exact reverses\n",
    "        if sum_comp(r) == sum_comp(p) and (r, p) <= (p, r):\n",
    "            r_str = ' + '.join(map(str, r))\n",
    "            p_str = ' + '.join(map(str, p))\n",
    "            if r_str == p_str:\n",
    "                continue  # skip trivial self‐reactions\n",
    "\n",
    "            # Compute ΔE in atomic units\n",
    "            dE_au = sum(df.loc[list(p), 'Energy']) - sum(df.loc[list(r), 'Energy'])\n",
    "            # Convert to kcal/mol (1 a.u. = 627.509 kcal/mol)\n",
    "            dE_kcal = dE_au * 627.509\n",
    "\n",
    "            # Reverse if positive to ensure negative ΔE\n",
    "            if dE_kcal > 0:\n",
    "                r_str, p_str = p_str, r_str\n",
    "                dE_kcal = -dE_kcal\n",
    "\n",
    "            rows.append({\n",
    "                'Reactants': r_str,\n",
    "                'Products': p_str,\n",
    "                'dE (kcal/mol)': round(dE_kcal, 1)\n",
    "            })\n",
    "result_df = pd.DataFrame(rows)\n",
    "print(result_df.to_string(index=False))\n",
    "\n",
    "# Save to CSV file\n",
    "output_path = '/Users/mantingmu/Desktop/elementary_reactions.csv' ### this is the input for the SPECI-AUTOCOPASI script for time vs conc. plot\n",
    "result_df.to_csv(output_path, index=False)\n",
    "print(f\"\\nResults saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8023bd8-0f01-4ab4-ab98-3ba4a5e060fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#************************************************* thank you for using SPECI ****************************************************\n",
    "#************************************************* enjoy the species!!! *********************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fae402-2c61-4230-9f90-eb38cb21646a",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "##### additional xtb function #####\n",
    "\n",
    "i = 0\n",
    "E_gas_xtb = []\n",
    "\n",
    "for G in GRAPHS:\n",
    "    a = str(i) \n",
    "    os.system(\"xtb --gfn 1 coord --chrg 0 speciation{}.xyz > temp-xtb/speciation{}.out\".format(a,a))\n",
    "    with open('temp-xtb/speciation' + str(i) + '.out', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines :\n",
    "            \n",
    "            if  'TOTAL ENERGY' in line:\n",
    "                words = line.split() \n",
    "                uffE = words[3]\n",
    "                break\n",
    "            \n",
    "            elif 'Program stopped due to fatal error' in line: \n",
    "                uffE = 'NA'\n",
    "                break\n",
    "        \n",
    "    E_gas_xtb.append(uffE)\n",
    "    i=i+1\n",
    "    print (i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
